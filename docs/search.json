[
  {
    "objectID": "hype-doc.html",
    "href": "hype-doc.html",
    "title": "Hype Document 2023",
    "section": "",
    "text": "#Goals for this year\n#Reflections\n#Projects (contributions, components, instights) (impact)\n#Collaboration & mentorship\n#Design & documentation\n#Company building (things that help the company overall, not just your team/project) Recruiting, improving important processes\n#What you learned\n#Outside of work Blog postes, open source, talks/ panels"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "I am a PhD Biologist working as a research geneticist. Currently, my research focuses on using deep learning to predict maize yield from genetic, environmental, and management data using deep learning, machine learning, and statistical modeling. Previously, I focused on compensation of neural circuits to aberrant activity."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Writings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRunning GWAS en masse.\n\n\n\ncode\n\n\nbash\n\n\nHPC\n\n\nSLURM\n\n\nintermediate\n\n\nGWAS\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJul 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a GWAS Container\n\n\n\ncode\n\n\nr\n\n\nbash\n\n\nintermediate\n\n\ncontainers\n\n\nGWAS\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip: Make your life easier with Symbolic Links in WSL\n\n\n\nbash\n\n\nbeginner\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuick Tip: Write notebooks, run scripts\n\n\n\nbeginner\n\n\ncode\n\n\nbash\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip: Use R to create an email nudge\n\n\n\ncode\n\n\nr\n\n\nbeginner\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nApr 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGo Read About R’s Function Call Semantics\n\n\n\ncode\n\n\nintermediate\n\n\nr\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMar 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorse is better case study 1\n\n\n\nbeginner\n\n\ncode\n\n\n\n\n\n\n\nDaniel Kick\n\n\nDec 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorse is better and not doing things “right”\n\n\n\ntacit knowledge\n\n\nbeginner\n\n\nintermediate\n\n\ncode\n\n\ndeep learning\n\n\n\n\n\n\n\nDaniel Kick\n\n\nOct 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse a spreadsheet to manage your CV & Resume\n\n\n\nprofessional development\n\n\nr\n\n\n\n\n\n\n\nDaniel Kick\n\n\nOct 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulation as a Super Power\n\n\n\ncode\n\n\nintermediate\n\n\nensembling\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMaking a “Visible” Neural Network\n\n\n\ncode\n\n\nadvanced\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolving the Wrong Problem\n\n\n\ncode\n\n\ndebugging\n\n\ntacit knowledge\n\n\nintermediate\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSave only what you need\n\n\n\ncode\n\n\ndebugging\n\n\ntacit knowledge\n\n\nbeginner\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCapturing Tacit Knowledge Through Blogging\n\n\n\ntacit knowledge\n\n\nbeginner\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: Do-nothing Scripting in Bash\n\n\n\ncode\n\n\nbash\n\n\nbeginner\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Comment is a Comment.\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrivia: R can have Comments in Tables\n\n\n\ncode\n\n\nr\n\n\nintermediate\n\n\ntips\n\n\ntrivia\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnsemble of BLUP, Machine Learning, and Deep Learning Models Predict Maize Yield Better Than Each Model Alone\n\n\n\n\n\n\nDaniel Kick\n\n\nApr 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYield prediction through integration of genetic, environment, and management data through deep learning\n\n\n\n\n\n\nDaniel Kick\n\n\nJan 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: Open a new Interactive Session in Running Session\n\n\n\ncode\n\n\nhpc\n\n\nintermediate\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nOct 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrivia: In Python Missing Isn’t Equal to Itself\n\n\n\ncode\n\n\npython\n\n\nbeginner\n\n\ntips\n\n\ntrivia\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTiming-Dependent Potentiation and Depression of Electrical Synapses Contribute to Network Stability in the Crustacean Cardiac Ganglion\n\n\n\n\n\n\nDaniel Kick\n\n\nAug 31, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot and the invisible hex code.\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\nggplot\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 31, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: Cache Intermediate Results with pickle\n\n\n\ncode\n\n\npython\n\n\nintermediate\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMar 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: For those coming from R: Silent In Place Replacement\n\n\n\ncode\n\n\npython\n\n\nr\n\n\nintermediate\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nFeb 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApproximate String Matching\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: Reusing Custom Functions\n\n\n\ncode\n\n\npython\n\n\nintermediate\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJul 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: More Readable Data with pretty-print\n\n\n\ncode\n\n\npython\n\n\nbeginner\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: Find the Graph you want in using a Graph Gallery\n\n\n\ncode\n\n\npython\n\n\nbeginner\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: Jupyter Plugins\n\n\n\ncode\n\n\npython\n\n\nbeginner\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Simulations to Check Your Statistical Intuition\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nsimulation\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMar 23, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot Reordering a Discrete Axis\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nggplot\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMar 11, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot: Parsing Expressions\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nggplot\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMar 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot tips: markdown for italics in plots and adding breaks to color scaling\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\nggplot\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMar 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorkaround for Plotting Dendrograms\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ndendrograms\n\n\n\n\n\n\n\nDaniel Kick\n\n\nFeb 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThought Experiment: Comparing Replicate Experiments’ Conclusions\n\n\n\ncode\n\n\nintermediate\n\n\nr\n\n\nresampling\n\n\n\n\n\n\n\nDaniel Kick\n\n\nFeb 3, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot frienndly corrlation plots with ggcorrplot\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nggplot\n\n\ncorrelations\n\n\n\n\n\n\n\nDaniel Kick\n\n\nFeb 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot font customization\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nggplot\n\n\n\n\n\n\n\nDaniel Kick\n\n\nDec 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling Local Libraries Post Update\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\n\n\n\n\n\nDaniel Kick\n\n\nDec 18, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiagrams as code\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nmermaid\n\n\ngraphviz\n\n\n\n\n\n\n\nDaniel Kick\n\n\nDec 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoman numeral convenince function\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nNov 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResampling doesn’t need to be hard\n\n\n\ncode\n\n\nintermediate\n\n\nr\n\n\nresampling\n\n\n\n\n\n\n\nDaniel Kick\n\n\nNov 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaching your enviroment and why you might not want to.\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Type Matters In gganimate\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\n\n\n\n\n\nDaniel Kick\n\n\nAug 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocument your functions with `roxygen2``\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJul 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReference current dataframe with .\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJul 28, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInformal profiling with tictoc\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nunlist is handy and you should use it\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot element_text() colors\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nggplot\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 5, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualize the Influence of Outliers\n\n\n\ncode\n\n\nintermediate\n\n\nr\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUpdating Results in Word from Table\n\n\n\ncode\n\n\nintermediate\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 26, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShading with geom_rect\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nggplot\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGreek Letters in ggplot\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nggplot\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDon’t Ignore Your .gitignore\n\n\n\ncode\n\n\nbeginner\n\n\ngit\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCache Intermediate Results\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidyverse ’_at’ variants\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 11, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSyntax Tours: learnxinyminutes\n\n\n\ncode\n\n\nbeginner\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 11, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot: Beware of Factors!\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 6, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUpgrading R versions on Windows\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApply a theme to all your ggplots\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nApr 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClearing all but certain objects\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nApr 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParallel Processing for purrr with furrr\n\n\n\ncode\n\n\nintermediate\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nApr 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoading R functions from source\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nApr 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMolecular profiling of single neurons of known identity in two ganglia from the crab Cancer borealis\n\n\n\n\n\n\nDaniel Kick\n\n\nDec 5, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCell Communication: Studying gap junctions with PARIS\n\n\n\n\n\n\nDaniel Kick\n\n\nMar 1, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDopamine maintains network synchrony via direct modulation of gap junctions in the crustacean cardiac ganglion\n\n\n\n\n\n\nDaniel Kick\n\n\nOct 16, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMotor Systems: Variability in neural networks\n\n\n\n\n\n\nDaniel Kick\n\n\nJan 18, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Hillary Climber trumps manual testing: an automatic system for studying Drosophila climbing\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 16, 2016\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/manuscript-lane-et-al-2018/index.html",
    "href": "posts/manuscript-lane-et-al-2018/index.html",
    "title": "Dopamine maintains network synchrony via direct modulation of gap junctions in the crustacean cardiac ganglion",
    "section": "",
    "text": "https://elifesciences.org/articles/39368"
  },
  {
    "objectID": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html",
    "href": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html",
    "title": "Running GWAS en masse.",
    "section": "",
    "text": "As part of a project I’m about to run a lot of analyses. Below are some initial tricks I’ve figured out with (hopefully) more posts like this to come as I find more effective patterns.\nThe goal here was to run a GWAS for some number of phenotypes using one or more genotype file and save all the output into a reasonably named subdirectory. Here’s the directory structure:\n.\n├── gapit.sif     # Discussed previously\n├── gwas.R        #\n├── 01_mkqueue.sh    # We'll focus on these two\n├── 02_next.sh       #    \n├── run_gwas.sbatch     # Loads apptainer and runs 02_next.sh\n│\n├── genotypes\n│   └── 5_Genotype_Data_All_Years_30000.hmp.txt\n│\n├── phenotypes\n│   ├── phno_Ear_Height_cm.txt\n│   ├── ... \n│   └── phno_Yield_Mg_ha.txt\n│\n└── output"
  },
  {
    "objectID": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#context",
    "href": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#context",
    "title": "Running GWAS en masse.",
    "section": "",
    "text": "As part of a project I’m about to run a lot of analyses. Below are some initial tricks I’ve figured out with (hopefully) more posts like this to come as I find more effective patterns.\nThe goal here was to run a GWAS for some number of phenotypes using one or more genotype file and save all the output into a reasonably named subdirectory. Here’s the directory structure:\n.\n├── gapit.sif     # Discussed previously\n├── gwas.R        #\n├── 01_mkqueue.sh    # We'll focus on these two\n├── 02_next.sh       #    \n├── run_gwas.sbatch     # Loads apptainer and runs 02_next.sh\n│\n├── genotypes\n│   └── 5_Genotype_Data_All_Years_30000.hmp.txt\n│\n├── phenotypes\n│   ├── phno_Ear_Height_cm.txt\n│   ├── ... \n│   └── phno_Yield_Mg_ha.txt\n│\n└── output"
  },
  {
    "objectID": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#design",
    "href": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#design",
    "title": "Running GWAS en masse.",
    "section": "Design",
    "text": "Design\nWe could modify gwas.R for each genotype/phenotype pair that needs to be run, create sbatch files for gwas_1.R to gwas_n.R and run them that way. This approach would be relatively fast1 to implement, but depends on 1) not making errors in editing these files and 2) isn’t a transferable solution to new datasets.\nLet’s start by breaking this down into manageable components:\n\nIdentify all the combinations of parameters that the script should be run with.\nPut those combinations in a queue\nUntil the queue is empty…\n\nPop an entry from from the queue\nEdit the R script to contain the new parameters\nRun the script in using the singularity container.\n\n\nThis approach is nice in that since we’re modifying a shared queue we can refer to the same files over and over again. Instead of ending up with n versions of gwas.R and n sbatch files we’ll have only one sbatch file that we’ll run again and again until the queue is empty."
  },
  {
    "objectID": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#building-a-queue-01_mkqueue.sh",
    "href": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#building-a-queue-01_mkqueue.sh",
    "title": "Running GWAS en masse.",
    "section": "Building a queue: 01_mkqueue.sh",
    "text": "Building a queue: 01_mkqueue.sh\nIn this example we have many phenotpe files but only one genotype file. Despite this, instead of hardcoding the genotype we’ll pretend we might have more at some future date (maybe we want to try different snp densities). We’ll build a simple queue by globbing all the hapmaps in genotypes/ and all the phnotype files in phenotypes/ and appending them to queue.txt.\nfor i in ./genotypes/*.hmp.txt; \ndo \n    for j in ./phenotypes/phno*.txt; \n    do  \n        echo $i $j &gt;&gt; ./queue.txt; \n    done; \ndone"
  },
  {
    "objectID": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#modifying-the-r-script-on-the-fly",
    "href": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#modifying-the-r-script-on-the-fly",
    "title": "Running GWAS en masse.",
    "section": "Modifying the R script on the fly",
    "text": "Modifying the R script on the fly\nEach line in queue.txt contains parameters we would like to run gwas.R with. There are different ways to accomplish this. For instance we could change gwas.R to read the top line from the queue file and parse it as parameters. This approach is one that I intend to use in the future but is not what I’ve done here. Here I’ve put the paths that the parameters will replace near the top of the file and changed nothing else.\nphno_file=\"./hmp_phno.txt\"\ngeno_file=\"./geno.hmp.txt\"\nFor each analysis I’ll duplicate this template file, move it to the output directory and then swap out these paths using sed. This works nicely and leaves a record of the code that was run but for more complex scripts might risk unintentionally editing lines you weren’t expecting to."
  },
  {
    "objectID": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#running-a-single-gwas-02_next.sh",
    "href": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#running-a-single-gwas-02_next.sh",
    "title": "Running GWAS en masse.",
    "section": "Running a single GWAS: 02_next.sh",
    "text": "Running a single GWAS: 02_next.sh\nNow we’re to the real core of the system. We need to manage the queue and modify and run the R script. We begin by reading the first line of the queue.txt file with head and using awk to grab the fields2 containing the genotype and phenotype paths.\ngeno_path=$(head -n 1 queue.txt |awk '{print $1}')\nphno_path=$(head -n 1 queue.txt |awk '{print $2}')\nNext we want to overwrite queue.txt without the first line. First we check how many lines are in the file with wc and then use tail to all but the top line. This updated queue we save to a temp file and then use mv to overwrite the queue file. We have to write an intermediate file because the pipe &gt; will activate before tail resulting queue being overwritten with an empty file.\n# find the lenght of the queue so it can be shortened\ntmp=($(wc --lines ./queue.txt))\nnlines=${tmp[0]}\n\n# This the pipe runs before tail so we have to use a temp file and then rename it.\ntail -n $((nlines -1)) queue.txt &gt; queue.tmp && mv queue.tmp queue.txt\nNow we can parse the paths into a directory name we can make use of. We’ll do this by using sed to replace text in this string to remove the directory information ( s|./.*/|| ) and file extension (e.g. s|\\.hmp\\.txt|| ). These file names we’ll concatenate into the name of the output directory ($save_dir).\n# create a new save location\n# remove the ./dir/ and .txt \nphno_name=$(echo $phno_path |sed 's|\\./.*/||' |sed 's|\\.txt||')\ngeno_name=$(echo $geno_path |sed 's|\\./.*/||' |sed 's|\\.hmp\\.txt||')\n\nsave_dir='./output/'$phno_name'__'$geno_name\nThe R script writes its output into the working directory so we’ll make a directory for this set of parameters, cd into it, and copy the template script in.\nmkdir $save_dir\n\n# change pwd \n# copy gwas.R to the save dir\ncd  './'$save_dir\ncp ../../gwas.R ./gwas.R\nUsing sed again, this time with the inplace option -i, we swap out the placeholder paths with a new path relative to the working directory.\n# modify the paths with a leading '.' to be '../'\n# use sed to replace the run settings\nsed -i 's|./hmp_phno.txt|../.'$phno_path'|' ./gwas.R\nsed -i 's|./geno.hmp.txt|../.'$geno_path'|' ./gwas.R\nWith those changes made we can execute this script using the singularity container gapit.sif in the root of the project.\nsingularity exec ../../gapit.sif Rscript ./gwas.R &gt; run.out\nHere’s the whole script all together.\n#!/bin/bash\n\ngeno_path=$(head -n 1 queue.txt |awk '{print $1}')\nphno_path=$(head -n 1 queue.txt |awk '{print $2}')\n\n# find the lenght of the queue so it can be shortened\ntmp=($(wc --lines ./queue.txt))\nnlines=${tmp[0]}\n\n# This the pipe runs before tail so we have to use a temp file and then rename it.\ntail -n $((nlines -1)) queue.txt &gt; queue.tmp && mv queue.tmp queue.txt\n\n# create a new save location\n# remove the ./dir/ and .txt \nphno_name=$(echo $phno_path |sed 's|\\./.*/||' |sed 's|\\.txt||')\ngeno_name=$(echo $geno_path |sed 's|\\./.*/||' |sed 's|\\.hmp\\.txt||')\n\nsave_dir='./output/'$phno_name'__'$geno_name\nmkdir $save_dir\n\n# change pwd \n# copy gwas.R to the save dir\ncd  './'$save_dir\ncp ../../gwas.R ./gwas.R\n\n# modify the paths with a leading '.' to be '../'\n# use sed to replace the run settings\nsed -i 's|./hmp_phno.txt|../.'$phno_path'|' ./gwas.R\nsed -i 's|./geno.hmp.txt|../.'$geno_path'|' ./gwas.R\n\nsingularity exec ../../gapit.sif Rscript ./gwas.R &gt; run.out"
  },
  {
    "objectID": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#putting-it-all-together",
    "href": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#putting-it-all-together",
    "title": "Running GWAS en masse.",
    "section": "Putting it all together",
    "text": "Putting it all together\nThe last thing we need to do is write a simple sbatch file to load apptainer (or singlularity) and run 02_next.sh. I’ve saved this as run_gwas.sbatch.\n# ... \n# Your node settings here\n# ...\n\nmodule load apptainer\nbash 02_next.sh\nNow we can put it all together. First run 01_mkqueue.sh to build the queue and then run sbatch run_gwas.sbatch over and over. Recycling the code from above to find the length of the queue we can set up a simple for loop to run sbatch the needed number of times.\n\nbash 01_mkqueue.sh\nn_runs=$(wc --lines ./queue.txt |awk '{print $1}')\nfor i in $(seq 1 $n_runs); \ndo  \n    echo $i\n    sbatch run_gwas.sbatch \ndone\nAnd that’s it! Now we let this churn away filling output/ with the nicely organized results for all our phenotypes."
  },
  {
    "objectID": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#footnotes",
    "href": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#footnotes",
    "title": "Running GWAS en masse.",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ni.e. faster than what I’m about to describe↩︎\nThere’s a space between them so `awk ’{print $1}` will return the text before the space.↩︎"
  },
  {
    "objectID": "posts/DanielKick/231003_profdev_r_for_cv/index.html",
    "href": "posts/DanielKick/231003_profdev_r_for_cv/index.html",
    "title": "Use a spreadsheet to manage your CV & Resume",
    "section": "",
    "text": "For the past year I’ve been using a spreadsheet and RStudio to manage my resume and curriculum vitae. This post is a pitch for why you might want to do this and an overview of the system. There will be a follow up post on how to get started if you decide to use his approach."
  },
  {
    "objectID": "posts/DanielKick/231003_profdev_r_for_cv/index.html#why-use-a-spreadsheet",
    "href": "posts/DanielKick/231003_profdev_r_for_cv/index.html#why-use-a-spreadsheet",
    "title": "Use a spreadsheet to manage your CV & Resume",
    "section": "Why use a spreadsheet?",
    "text": "Why use a spreadsheet?\nThe key reason why you should use a spreadsheet to manage your resume is that it enables easy filtering and sorting. In essence your spreadsheet becomes a personal work history database from which you can quickly retrieve entries relevant to the application at hand.\nIf instead all your experiences went into an all encompassing text document, tailoring a document to a position would require you to go through each section and cut out most of the entries. To be clear – there are far worse strategies out there. Having a single reference document keeps your information together and means that much of your formatting work is done ahead of time.\nThis is where RStudio comes in. Rmarkdown gives you a way to draw entries from your spreadsheet, filter them, and then turn those entries into beautifully formatted text. Not to mention that you can have R update text for you1."
  },
  {
    "objectID": "posts/DanielKick/231003_profdev_r_for_cv/index.html#overview",
    "href": "posts/DanielKick/231003_profdev_r_for_cv/index.html#overview",
    "title": "Use a spreadsheet to manage your CV & Resume",
    "section": "Overview",
    "text": "Overview\nLet’s see how this works. All the code and data for my resume and cv is on GitHub. Every time I update the spreadsheet or tweak the documents’ aesthetics I add a commit.\n\nUpdating the documents is a 4 step process that takes just a few moments. Earlier this month when a paper of mine was accepted. To update my documents I…\n\nedited the positions.csv document and added a row with the paper’s bibliographic information and url.\nopened my Rmarkdown files Curriculum-Vitae.Rmd and Resume.Rmd and knit them to pdfs.\ncopied the output pdfs to a second repository to share with others\ncommited and pushed these new documents to GitHub (and then did the same for the cv repository)\n\n\nYou might be wondering – what’s with the last two steps? Why move the documents and then push to GitHub instead of keeping them where they are? And why put these on GitHub in the first place?\nPutting these on GitHub (or online for that matter) makes it easy to go from your resume to your online presence (LinkedIn, Orcid, personal website, etc.) from links in your resume. This means that if you embed a link to your resume in your resume then every paper copy you hand out and every business card links to the most up to date version.\n\nUsing a second repository is just to have a cleaner presentation. Every file that isn’t your resume is a distraction – and if your recipient wants to see your GitHub they’ll be just one click away."
  },
  {
    "objectID": "posts/DanielKick/231003_profdev_r_for_cv/index.html#a-more-detailed-look",
    "href": "posts/DanielKick/231003_profdev_r_for_cv/index.html#a-more-detailed-look",
    "title": "Use a spreadsheet to manage your CV & Resume",
    "section": "A More Detailed Look:",
    "text": "A More Detailed Look:\nLet’s take a look at positions.csv.\nThe first column, section, is the categories in your document. Several categories might be presented together (e.g. national_presentations and regional_presentations) but they aren’t assumed to be. The next column, in_resume, is a simple filter. Everything goes into the curriculum vitae, not so with the resume. Next we have institution. This one is a little odd because it includes university or organization names but also lists of authors (e.g. row 44). This is because all the items in this column are formatted the same way. After showing the title of an entry, we want to show this information. Dates are included using the start and end columns and location information (here urls) in loc. Finally, are several description_ columns. Extra information you want can be added to these.\n\nWhen I updated this spreadsheet I already had an entry from when I submitted the paper and put it $bioR\\chiiv$.\n\nOnly three cells had to change, moving it from the in review section to the academic articles section, tweaking the title, and updating the url.\n\nIn RStudio, the CV is ready to go. All I had to do was click the Knit button and wait for the pdf.\nThe resume took ever so slightly more work. For space and aesthetic reasons I use a non-default formatting for my publications. This is a manual step but is not hard at all. All I did was:\n\nRead in the position data and run line 433 position_data %&gt;% print_section('academic_articles'). This produces markdown formatted text. Each line of text is treated as a separate “item” and will be formatted according to some rules.\nCopy the markdown formatted text for the new publication.\nPaste it into the document and tweak the formatting:\n\nUse a smaller font size for everything between &lt;font size=\"1\"&gt; and &lt;/font&gt; .\nBold my name using ***\n\nDisplay authors and link together as the second line and don’t apply the third line’s formatting rules to anything (N/A)\n\n\n\n\nWith those edits made I click the “Knit” button again et voilà! Resume updated and ready to go."
  },
  {
    "objectID": "posts/DanielKick/231003_profdev_r_for_cv/index.html#bonus-updating-values-in-the-text",
    "href": "posts/DanielKick/231003_profdev_r_for_cv/index.html#bonus-updating-values-in-the-text",
    "title": "Use a spreadsheet to manage your CV & Resume",
    "section": "Bonus: Updating values in the text",
    "text": "Bonus: Updating values in the text\nOne of the coolest tricks you can do if your resume is in R is to update text dynamically. You don’t have to search through and count how many students you’ve mentored, or papers you published2.\nHere’s a simple example. I want to include how long I’ve been using R, but I don’t want to have to update that by hand. You could calculate this in R like so:\n\ntoday     &lt;- as.Date(format(Sys.time(), \"%Y-%m-%d\")) # Get today's date\nstart_R   &lt;- as.Date(\"2017-01-29\")                   # Set starting date\ndays_diff &lt;- difftime(today, start_R)                # Calc. days elapsed\nyears     &lt;- as.numeric(days_diff) / 365             # Convert to years\nyears     &lt;- round(years)                            # Round\nyears\n\nRMarkdown let’s you embed this calculation in the text. In my documents I have something like this “R Programming (7years )” which will show up in the pdf as “R Programming (# years)”."
  },
  {
    "objectID": "posts/DanielKick/231003_profdev_r_for_cv/index.html#footnotes",
    "href": "posts/DanielKick/231003_profdev_r_for_cv/index.html#footnotes",
    "title": "Use a spreadsheet to manage your CV & Resume",
    "section": "Footnotes",
    "text": "Footnotes\n\nSee the bonus at the very end.↩︎\nLook at line 430 in Resume.Rmd↩︎"
  },
  {
    "objectID": "posts/DanielKick/220523_r_comments_in_tables/index.html",
    "href": "posts/DanielKick/220523_r_comments_in_tables/index.html",
    "title": "Trivia: R can have Comments in Tables",
    "section": "",
    "text": "R allows for comments to exist in tables. If there’s a # in the table you’re reading (e.g. as a part of a column name like chromosome#) then it can cause an unequal number of values between rows (everything on that line following it is ignored). The solution is to specify the comment character explicitly to be used (it can be ’’ to have no comment characters). Here’s an example:\necho \"a, b, c#, d\" &gt; test_table.txt\n&gt; Rscript -e \"read.table('test_table.txt')\"\n#   V1 V2 V3\n# 1 a, b,  c\n&gt; Rscript -e \"read.table('test_table.txt', comment.char = '')\" # with no comment character, all entries will be read\n#   V1 V2  V3 V4\n# 1 a, b, c#,  d"
  },
  {
    "objectID": "posts/DanielKick/231013_premature_optimiztion_worse_is_better/index.html",
    "href": "posts/DanielKick/231013_premature_optimiztion_worse_is_better/index.html",
    "title": "Worse is better and not doing things “right”",
    "section": "",
    "text": "“Worse is better” is an idea I get a lot of mileage out of. Here’s the crux of it:\nI find this is useful descriptively1 but also prescriptively as a way to spend less time doing work that doesn’t need to be done.\nIn brief the idea is that once you have something that works it’s often not worth altering it to make it faster, more efficient, or more elegant … at least initially. Optimization is important (example but what I’m talking about here premature optimization. Avoiding the urge to improve things that aren’t the priority can be difficult, especially when you conceptually know what you would change."
  },
  {
    "objectID": "posts/DanielKick/231013_premature_optimiztion_worse_is_better/index.html#simplified-example",
    "href": "posts/DanielKick/231013_premature_optimiztion_worse_is_better/index.html#simplified-example",
    "title": "Worse is better and not doing things “right”",
    "section": "Simplified Example",
    "text": "Simplified Example\nHere’s an example: I’m building a network that ‘compresses’ information. The key idea is that there’s a function, f(), takes in some number of values and outputs fewer values. We can use this function over and over again to compress the values more and more. Once they’re ‘compressed’ we can do the reverse procedure and get more values until we’re back at the starting number.\nThere’s a catch however, and that’s that the function can only output an integer number of values even if the number should be a fraction. It’s like this division function. If the numerator argument is the number of input values and it will return numerator/3 values.\n\ndiv_it &lt;- function(numerator, divisor = 3){\n  res = numerator/divisor\n  res = round(res)\n  return(res)\n}\n\ndiv_it(100)\n\n[1] 33\n\n\nBecause it can only return whole numbers, we can’t reverse this procedure and always get back the same number – sometimes we have to add or subtract a little bit.\n\ninv_div_it &lt;- function(numerator, divisor = 3){\n  return(numerator*divisor)\n}\n\ninv_div_it(33)\n\n[1] 99\n\ninv_div_it(33)+1\n\n[1] 100\n\n\nIf we want to really compress the input (f(X) |&gt; f(X) |&gt; f(X) |&gt; f(X) or f(f(f(f(X))))) then the number of values at each level would be:\n\nvals &lt;- c(100)\nfor(i in 1:4){\n  i_val &lt;- vals[length(vals)]\n  vals[length(vals)+1] &lt;- div_it(i_val) \n}\nvals\n\n[1] 100  33  11   4   1\n\n\nIdeally running the inverse procedure multiple times on the last output above (just one value) would output produce:\n\nvals_reverse &lt;- vals[length(vals):1]\nvals_reverse\n\n[1]   1   4  11  33 100\n\n\nBut using the inverse function defined above (inv_div_it()) we get:\n\nrecover_vals &lt;- c(1)\nfor(i in 1:4){\n  i_val &lt;- recover_vals[length(recover_vals)]\n  recover_vals[length(recover_vals)+1] &lt;- inv_div_it(i_val) \n}\nrecover_vals\n\n[1]  1  3  9 27 81\n\n\nTo get back to 100 values we need to add a new value (imagine appending a 1 to an array) sometimes, and drop a value others, or make no change to output other times.\n\nadd_vals &lt;- c(1, -1, 0, 1)\n\nrecover_vals &lt;- c(1)\nfor(i in 1:4){\n  i_val &lt;- recover_vals[length(recover_vals)]\n  print(add_vals[i])\n  recover_vals[length(recover_vals)+1] &lt;- inv_div_it(i_val) + add_vals[i]\n}\n\n[1] 1\n[1] -1\n[1] 0\n[1] 1\n\nrecover_vals\n\n[1]   1   4  11  33 100\n\n\nWe could keep track of the remainder each time f() is called and use that to figure out when to add or subtract 1. That would be the elegant and efficient solution. We know the desired output (100 values) and the number of times f() was called (4) so we could also try changing the numbers in add_vals until we have four numbers that. This solution would be inelegant but still effective.\nIf a piece of code only needs to be a few times then the cost of the time you’d spend optimizing it will probably be worth more than than cost of the time the computer spends running it (see also).\nIf the sloppy way to express what you want is good enough then don’t worry about it. Good enough now is often better than perfect later."
  },
  {
    "objectID": "posts/DanielKick/231013_premature_optimiztion_worse_is_better/index.html#example-in-context-python",
    "href": "posts/DanielKick/231013_premature_optimiztion_worse_is_better/index.html#example-in-context-python",
    "title": "Worse is better and not doing things “right”",
    "section": "Example in Context (python)",
    "text": "Example in Context (python)\nThe motivating problem behind this write up is that ‘compressing’ weather data (17 measurements for 365 days) into fewer values. I’m using a variation autoencoder with convolution layers which you can imagine as passing a sliding window over a 17x365 grid and summarizing each windowed chunk to get fewer values.\nTo check if the compression is effective, we have to compress 17x365 values down to something smaller (e.g. 17x23), and inflate them back to 17x365 so we can compare the input weather to the output weather. If we can get back the same 17x365 values (or something pretty close) then the comprssion is effective..\nFrom the input data’s length (days) you can calculate what a convolutional layer’s output length will be like so:\ndef L_out_conv1d(\n    L_in = 365, \n    kernel_size=3, stride = 2, padding=1, dilation = 1\n): return ((L_in +2*padding-dilation*(kernel_size-1)-1)/stride)+1\n\nL_out_conv1d(L_in = 365) # 183.0\nAnd the same for reversing the operation (with a transposed convolution).\ndef L_out_convT1d(\n    L_in = 183, \n    kernel_size=3, stride = 2, padding=1, output_padding=0, dilation = 1\n): return (L_in - 1)*stride-2*padding+dilation*(kernel_size-1)+output_padding+1\n\nL_out_convT1d(L_in = 183) # 365.0\nThe trouble is that if I stack convolution layers the output length can become a fraction, which is forced to an integer, and prevents the reverse operation from producing the right number. When I use 4 layers the length should be [365, 183.0, 92.0, 46.5, 23.75] which as integers is [365, 183, 92, 46, 23]. Reversing the operation produces [23, 45, 89, 177, 353].\nWe can get back to 365 days by increasing the output’s length in some of the transposed convolution layers by adding a non-zero output_padding. I don’t know how many layers will be best, so I can’t hard code these values. I could use the functions above to calculate what when the output_padding should be 0 and when it shouldn’t (the elegant solution), but that’s not what I did.\nInstead I made a simple disposable neural network just to check if I had the output_paddings right by tracking the lengths of the tensor after each layer.\n# input data. One observation, 17 measurements, 365 days of measurements. \n# It's all 0s because all I care about right now is the dimensions of the data.\nxin = torch.zeros((1, 17, 365))\n\n# Proposed output_padding for each layer in the decoder network\nlayer_output_padding = [0, 0, 0, 0, 0, 0, 0, 0]\n\n# encoder network\nne = nn.ModuleList([\n    nn.Sequential(nn.Conv1d(\n    17, out_channels=17, kernel_size= 3, stride= 2, padding  = 1), nn.BatchNorm1d(\n    17), nn.LeakyReLU())\n    for i in range(len(layer_output_padding))\n])\n\n# Decoder network\nnd = nn.ModuleList([\n    nn.Sequential(nn.ConvTranspose1d(\n    17, 17, \n    kernel_size=3, stride = 2, padding=1, output_padding=layer_output_padding[i]), nn.BatchNorm1d(\n    17), nn.LeakyReLU())\n    for i in range(len(layer_output_padding))\n])\nThen I can run this network …\n# list to store lengths\ntensor_Ls = []\n\n# add the input data's length (days)\ntensor_Ls += [list(xin.shape)[-1]] \n\n# encode data\nfor mod in ne:\n    xin = mod(xin)\n    tensor_Ls += [list(xin.shape)[-1]]\n\n# add the encoded data's \ntensor_Ls += [str(tensor_Ls[-1])]\n\n# decode data\nfor mod in nd:\n    xin = mod(xin)\n    tensor_Ls += [list(xin.shape)[-1]]\n… and look at the first and last value of the list of lengths (tensor_Ls) to see if the proposed output paddings will work.\ntensor_Ls[0] == tensor_Ls[-1]\n# False\ntensor_Ls\n# [365, 183, 92, 46, 23, 12, 6, 3, 2, '2', 3, 5, 9, 17, 33, 65, 129, 257]\nNext I need a way to systematically produce different paddings. For a decoder of four layers I would test paddings [0, 0, 0, 0], [1, 0, 0, 0], ... [1, 1, 1, 1] stopping at the first list that works. So I’ll write a function to increment [0, 0, 0, 0] to [1, 0, 0, 0].\ndef increment_list(\n    in_list = [0, 0, 0, 0],\n    min_value = 0,\n    max_value = 1):\n    # Check that all entries are within min/max\n    if False in [True if e &lt;= max_value else False for e in in_list]:\n        print('Value(s) above maximum!')\n    elif False in [True if e &gt;= min_value else False for e in in_list]:\n        print('Value(s) below minimum!')\n    elif [e for e in in_list if e != max_value] == []:\n        print('List at maximum value!')\n    else:    \n        # start cursor at first non-max value\n        for i in range(len(in_list)):\n            if in_list[i] &lt; max_value:\n                in_list[i] += 1\n                break\n            else:\n                in_list[i] = min_value\n    return(in_list)\n\nincrement_list()\n# [1, 0, 0, 0]\nThen we can loop through possible paddings until we find one that works or have tried all of them.\n# Proposed output_padding for each layer in the decoder network\nlayer_output_padding = [0, 0, 0, 0, 0, 0, 0, 0]\n\nwhile True:\n    # save a backup of the current padding\n    old_layer_output_padding = layer_output_padding.copy()\n    \n    \n    # ... define and run network here ...\n    \n    \n    # If it did work we're done\n    if True == (tensor_Ls[0] == tensor_Ls[-1]):\n        print('done!')\n        \n    # If the padding _didn't_ work change it\n    else:\n        layer_output_padding = increment_list(\n            in_list = layer_output_padding,\n            min_value = 0,\n            max_value = 1)\n    \n    # If the proposed new padding is the same as the backup, then we have tried all the possible paddings and will stop. \n    if layer_output_padding == old_layer_output_padding: \n        break\nAll together\nHere’s the full loop and its output:\n\n# Proposed output_padding for each layer in the decoder network\nlayer_output_padding = [0, 0, 0, 0, 0, 0, 0, 0]\n\nwhile True:\n    # save a backup of the current padding\n    old_layer_output_padding = layer_output_padding.copy()\n    \n    # input data. One observation, 17 measurements, 365 days of measurements. \n    # It's all 0s because all I care about right now is the dimensions of the data.\n    xin = torch.zeros((1, 17, 365))\n\n    # encoder network\n    ne = nn.ModuleList([\n        nn.Sequential(nn.Conv1d(\n        17, out_channels=17, kernel_size= 3, stride= 2, padding  = 1), nn.BatchNorm1d(\n        17), nn.LeakyReLU())\n        for i in range(len(layer_output_padding))\n    ])\n\n    # Decoder network\n    nd = nn.ModuleList([\n        nn.Sequential(nn.ConvTranspose1d(\n        17, 17, \n        kernel_size=3, stride = 2, padding=1, output_padding=layer_output_padding[i]), nn.BatchNorm1d(\n        17), nn.LeakyReLU())\n        for i in range(len(layer_output_padding))\n    ])\n    \n    # list to store lengths\n    tensor_Ls = []\n\n    # add the input data's length (days)\n    tensor_Ls += [list(xin.shape)[-1]] \n\n    # encode data\n    for mod in ne:\n        xin = mod(xin)\n        tensor_Ls += [list(xin.shape)[-1]]\n\n    # add the encoded data's \n    tensor_Ls += [str(tensor_Ls[-1])]\n\n    # decode data\n    for mod in nd:\n        xin = mod(xin)\n        tensor_Ls += [list(xin.shape)[-1]]    \n    \n    # If it did work we're done\n    if True == (tensor_Ls[0] == tensor_Ls[-1]):\n        print('done!')\n        \n    # If the padding _didn't_ work change it\n    else:\n        layer_output_padding = increment_list(\n            in_list = layer_output_padding,\n            min_value = 0,\n            max_value = 1)\n    \n    # If the proposed new padding is the same as the backup, then we have tried all the possible paddings and will stop. \n    if layer_output_padding == old_layer_output_padding: \n        break\n\n# done!\n\nlayer_output_padding  \n# [0, 1, 1, 0, 1, 1, 0, 0]\n\ntensor_Ls\n# [365, 183, 92, 46, 23, 12, 6, 3, 2, '2', 3, 6, 12, 23, 46, 92, 183, 365]\nThis may not be as not as elegant or as efficient as it could be, but it doesn’t matter. It only takes about 200ms so it’s not worth improving unless."
  },
  {
    "objectID": "posts/DanielKick/231013_premature_optimiztion_worse_is_better/index.html#footnotes",
    "href": "posts/DanielKick/231013_premature_optimiztion_worse_is_better/index.html#footnotes",
    "title": "Worse is better and not doing things “right”",
    "section": "Footnotes",
    "text": "Footnotes\n\ne.g. If scientific manuscripts with embedded code are valuable for reproducibility, why haven’t they become the default? There’s a lot of energy needed to switch and all of your collaborators already know word. 🤷🏼‍♂ ↩︎"
  },
  {
    "objectID": "posts/DanielKick/210615_python_graph_gallery/index.html",
    "href": "posts/DanielKick/210615_python_graph_gallery/index.html",
    "title": "Tips: Find the Graph you want in using a Graph Gallery",
    "section": "",
    "text": "This or similar sites can be helpful for looking up the right code/library for a plot. You can also find library specific ones. (matplotlib, plotly)\nOne of R’s main plotting libraries, ggplot2, describes plots by layering one component on top of another (e.g. starting with x and y variables, adding points, adding error bars, adding aesthetic adjustments). If that sort of approach appeals to you there is a python version of this library called plotnine (github, example use)."
  },
  {
    "objectID": "posts/DanielKick/210621_python_data_readability/index.html",
    "href": "posts/DanielKick/210621_python_data_readability/index.html",
    "title": "Tips: More Readable Data with pretty-print",
    "section": "",
    "text": "Here’s a tool that some may find useful when working with data that’s not yet in a [DataFrame]. It lets one “pretty-print” an object making any text that would wrap easier to read.\n# [In]\nprint(results_dictionary)\nprint(\"\\n --------------------------------------------- \\n\")\nimport pprint\npprint.PrettyPrinter(indent=4).pprint(results_dictionary)\n# [Out]\n{'active': True, 'additionalInfo:programDbId': '343', 'additionalInfo:programName': 'UC Davis', 'commonCropName': 'Cassava', 'contacts': None, 'culturalPractices': None, 'dataLinks': [], \n# ...\n'trialDbId': '343', 'trialName': 'UC Davis'}\n--------------------------------------------- \n{   'active': True,\n    'additionalInfo:programDbId': '343',    'additionalInfo:programName': 'UC Davis',    'commonCropName': 'Cassava',    'contacts': None,    'culturalPractices': None,    'dataLinks': [],    # ...    'trialDbId': '343',    'trialName': 'UC Davis'}"
  },
  {
    "objectID": "posts/DanielKick/index.html",
    "href": "posts/DanielKick/index.html",
    "title": "Writings",
    "section": "",
    "text": "Posts here written for at target audience of graduate students and undergraduates. Most are available on the lab protocol website of my postdoctoral lab.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRunning GWAS en masse.\n\n\n\ncode\n\n\nbash\n\n\nHPC\n\n\nSLURM\n\n\nintermediate\n\n\nGWAS\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJul 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a GWAS Container\n\n\n\ncode\n\n\nr\n\n\nbash\n\n\nintermediate\n\n\ncontainers\n\n\nGWAS\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip: Make your life easier with Symbolic Links in WSL\n\n\n\nbash\n\n\nbeginner\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuick Tip: Write notebooks, run scripts\n\n\n\nbeginner\n\n\ncode\n\n\nbash\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip: Use R to create an email nudge\n\n\n\ncode\n\n\nr\n\n\nbeginner\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nApr 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGo Read About R’s Function Call Semantics\n\n\n\ncode\n\n\nintermediate\n\n\nr\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMar 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorse is better case study 1\n\n\n\nbeginner\n\n\ncode\n\n\n\n\n\n\n\nDaniel Kick\n\n\nDec 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorse is better and not doing things “right”\n\n\n\ntacit knowledge\n\n\nbeginner\n\n\nintermediate\n\n\ncode\n\n\ndeep learning\n\n\n\n\n\n\n\nDaniel Kick\n\n\nOct 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse a spreadsheet to manage your CV & Resume\n\n\n\nprofessional development\n\n\nr\n\n\n\n\n\n\n\nDaniel Kick\n\n\nOct 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulation as a Super Power\n\n\n\ncode\n\n\nintermediate\n\n\nensembling\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMaking a “Visible” Neural Network\n\n\n\ncode\n\n\nadvanced\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolving the Wrong Problem\n\n\n\ncode\n\n\ndebugging\n\n\ntacit knowledge\n\n\nintermediate\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSave only what you need\n\n\n\ncode\n\n\ndebugging\n\n\ntacit knowledge\n\n\nbeginner\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCapturing Tacit Knowledge Through Blogging\n\n\n\ntacit knowledge\n\n\nbeginner\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: Do-nothing Scripting in Bash\n\n\n\ncode\n\n\nbash\n\n\nbeginner\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Comment is a Comment.\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrivia: R can have Comments in Tables\n\n\n\ncode\n\n\nr\n\n\nintermediate\n\n\ntips\n\n\ntrivia\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: Open a new Interactive Session in Running Session\n\n\n\ncode\n\n\nhpc\n\n\nintermediate\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nOct 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrivia: In Python Missing Isn’t Equal to Itself\n\n\n\ncode\n\n\npython\n\n\nbeginner\n\n\ntips\n\n\ntrivia\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot and the invisible hex code.\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\nggplot\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 31, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: Cache Intermediate Results with pickle\n\n\n\ncode\n\n\npython\n\n\nintermediate\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMar 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: For those coming from R: Silent In Place Replacement\n\n\n\ncode\n\n\npython\n\n\nr\n\n\nintermediate\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nFeb 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApproximate String Matching\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: Reusing Custom Functions\n\n\n\ncode\n\n\npython\n\n\nintermediate\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJul 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: More Readable Data with pretty-print\n\n\n\ncode\n\n\npython\n\n\nbeginner\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: Find the Graph you want in using a Graph Gallery\n\n\n\ncode\n\n\npython\n\n\nbeginner\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: Jupyter Plugins\n\n\n\ncode\n\n\npython\n\n\nbeginner\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Simulations to Check Your Statistical Intuition\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nsimulation\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMar 23, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot Reordering a Discrete Axis\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nggplot\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMar 11, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot: Parsing Expressions\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nggplot\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMar 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot tips: markdown for italics in plots and adding breaks to color scaling\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\nggplot\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMar 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorkaround for Plotting Dendrograms\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ndendrograms\n\n\n\n\n\n\n\nDaniel Kick\n\n\nFeb 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThought Experiment: Comparing Replicate Experiments’ Conclusions\n\n\n\ncode\n\n\nintermediate\n\n\nr\n\n\nresampling\n\n\n\n\n\n\n\nDaniel Kick\n\n\nFeb 3, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot frienndly corrlation plots with ggcorrplot\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nggplot\n\n\ncorrelations\n\n\n\n\n\n\n\nDaniel Kick\n\n\nFeb 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot font customization\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nggplot\n\n\n\n\n\n\n\nDaniel Kick\n\n\nDec 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling Local Libraries Post Update\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\n\n\n\n\n\nDaniel Kick\n\n\nDec 18, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiagrams as code\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nmermaid\n\n\ngraphviz\n\n\n\n\n\n\n\nDaniel Kick\n\n\nDec 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoman numeral convenince function\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nNov 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResampling doesn’t need to be hard\n\n\n\ncode\n\n\nintermediate\n\n\nr\n\n\nresampling\n\n\n\n\n\n\n\nDaniel Kick\n\n\nNov 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaching your enviroment and why you might not want to.\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Type Matters In gganimate\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\n\n\n\n\n\nDaniel Kick\n\n\nAug 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocument your functions with `roxygen2``\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJul 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReference current dataframe with .\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJul 28, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInformal profiling with tictoc\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nunlist is handy and you should use it\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot element_text() colors\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nggplot\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 5, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualize the Influence of Outliers\n\n\n\ncode\n\n\nintermediate\n\n\nr\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUpdating Results in Word from Table\n\n\n\ncode\n\n\nintermediate\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 26, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShading with geom_rect\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nggplot\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGreek Letters in ggplot\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nggplot\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDon’t Ignore Your .gitignore\n\n\n\ncode\n\n\nbeginner\n\n\ngit\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCache Intermediate Results\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidyverse ’_at’ variants\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 11, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSyntax Tours: learnxinyminutes\n\n\n\ncode\n\n\nbeginner\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 11, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot: Beware of Factors!\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 6, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUpgrading R versions on Windows\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApply a theme to all your ggplots\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nApr 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClearing all but certain objects\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nApr 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParallel Processing for purrr with furrr\n\n\n\ncode\n\n\nintermediate\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nApr 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoading R functions from source\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nApr 1, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/DanielKick/230524_linux_do_nothing_scripting/index.html",
    "href": "posts/DanielKick/230524_linux_do_nothing_scripting/index.html",
    "title": "Tips: Do-nothing Scripting in Bash",
    "section": "",
    "text": "Do-nothing scripting is a nice way to blend documenting a protocol with running it. You can use this template as a place to start:\n#!/usr/bin/bash\n#-----------------------------------------------------------------------------#\nSTEP='Step 0:'\necho \"$STEP\"\necho \"Run? (y/n)\"; read -n 1 k &lt;&1\nif [[ $k = n ]] ; then\nprintf \"\\nSkipping $STEP\\n\"; fi\nelse\nprintf \"\\nDoing $STEP\\n\"\n# Code for step here:\nNote, having the condition be on n instead of yes allows for the code (which will vary in length) to be at the end. This makes the control flow easy to see."
  },
  {
    "objectID": "posts/DanielKick/220928_python_trivia_missing/index.html",
    "href": "posts/DanielKick/220928_python_trivia_missing/index.html",
    "title": "Trivia: In Python Missing Isn’t Equal to Itself",
    "section": "",
    "text": "Python quirk I just learned and think is worth sharing. A missing valued doesn’t equal itself.\nHere’s the context: I’m making a list of values from a column that could not be converted to a date. Missing values can’t be converted so they end up in the list (e.g. [nan, '7/5/21 for pass 2']. So how do we discard this empty value? We use a list comprehension to see if the value is equal to itself ( [val for val in my_list if val == val] ) and will get a nan free list."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200421_tips_ggplot_theme_default/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200421_tips_ggplot_theme_default/index.html",
    "title": "Apply a theme to all your ggplots",
    "section": "",
    "text": "If you’re applying the same theme to all your graphs, set it globally instead e.g. theme_set(ggplot2::theme_minimal()). If you have a lot of custom changes to your theme, throw those into a function and set that to the global theme."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/210311_ggplot_discrete_axis_ordering/index.html",
    "href": "posts/DanielKick/210000_phd_tips/210311_ggplot_discrete_axis_ordering/index.html",
    "title": "ggplot Reordering a Discrete Axis",
    "section": "",
    "text": "Reordering a discrete axis in ggplot after generation a lot simpler than one might expect. Rather than converting a character column to a factor (what if the data gets pivoted?), or using one column for the position and one for the labels, you can use xlim or ylim.\n&gt; mrna_cols \\# desired order \\# \\[1\\] \"nav\" \"cav1\" \"cav2\" \"bkkca\"\n&gt; \"shaker\" \"shal\" \"shab\" \"shaw1\" \\# \\[9\\] \"shaw2\" \"inx1\" \"inx2\" \"inx3\"\n\no_mrna$heatmap_z / # ggplot object within a list\no_mrna$heatmap_z+ylim(mrna_cols)\n\n\n\nimage (31).png"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200919_tips_r_cache_env/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200919_tips_r_cache_env/index.html",
    "title": "Caching your enviroment and why you might not want to.",
    "section": "",
    "text": "If you’re working on data that takes a long time to process, consider adding a the following to your analysis.\nsave.image(file='myEnvironment.RData')\nload('myEnvironment.RData')\nThis will let you reload your environment. One can also by default save your environment when closing Rstudio but that may make it easy to reference objects that no longer are generated in the document itself thereby speeding software rot."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200511_tips_learnxiny/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200511_tips_learnxiny/index.html",
    "title": "Syntax Tours: learnxinyminutes",
    "section": "",
    "text": "Need a quick base R syntax lookup? Check out https://learnxinyminutes.com/docs/r/ . There are even examples with lm() and glm()."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200612_tips_r_use_unlist/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200612_tips_r_use_unlist/index.html",
    "title": "unlist is handy and you should use it",
    "section": "",
    "text": "Y’all should be using unlist(). unlist is a crazy handy function when you pair it with the purrr library. It’ll take a list and try to give you a vector. Strictly speaking it’s probably best to be using map_dbl() or map_chr() instead but let’s not worry about that at the moment.\nWhy do I love unlist so much? Because unlist(map( )) gives you a flexible, effective way to iterate (that is parallel friendly with minor changes).\nHere’s an example: I have a bunch of traces in a folder and I need to know if there are any experiments that didn’t copy. Additionally, I’d like to know what kind of data is in the file. I could look through them one by one, but that’s no fun. Thankfully, R has functions that perform similarly to shell functions. (More on these some other time.)\nSo we start by defining a data.frame with all file names and their sizes. Note that calling unlist(map()) inside data.frame() lets us do a lot work very quickly. We find all the files, get information about each one, and then selectively return the size.\ntraces_dir &lt;- \"C:/Users/Daniel/Documents/Trace_Holding\"\n\nfiles_df &lt;- data.frame(files = list.files(traces_dir),\n                       bytes = unlist(map(list.files(traces_dir), \n                                          function(abf){ &lt;http://file.info|file.info&gt;(paste0(traces_dir, \"/\",abf))$size })))\n\n#              files    bytes\n# 1 190808a_0000.abf 15006208 &lt;- long gap free recording\n# 2 190808a_0001.abf 15006208\n# 3 190808a_0002.abf 15006208\nThe experiment is embedded in the file name for each. Once again , with a little help from unlist we can split the file names into a list of list (i.e. \"190808a_0000.abf\" becomes [[1]] [[1]] \"190808a\" [[2]] \"0000.abf\" select only the first part and populate a new column.\nfiles_df$Experiment &lt;- unlist(transpose(str_split(files_df$files, pattern = \"_\"))[[1]])\nOkay, now we can make use of this. I’ve defined a data.frame for metadata about the experiments.\nfile_groups\n#    Experiment       Group\n# 1      190924    Baseline\n# ... \n# 19     190918     Delayed\nWe can join these data frames and apply a little tidyverse magic to see what experiments are missing (we could also compare the sets of experiments directly).\nfull_join(file_groups, files_df) %&gt;% \n  filter(&lt;http://is.na|is.na&gt;(files)) %&gt;% \n  group_by(Group, Experiment) %&gt;% \n  tally()\n\n#  Experiments that didn't transfer:\n\n#   Group       Experiment     n\n# 1 Baseline    190924         1\n# 2 Baseline    190924a        1\n# ...\nWe can repeat the same strategy to programattically look at the protocol types (e.g. based on file size or channel number via &lt;http://file.info|file.info&gt; | readABF()). Moral of the story, you should so stop applying yourself and give unlist and purrr functions a try."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200728_tips_r_tidyverse_current_df/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200728_tips_r_tidyverse_current_df/index.html",
    "title": "Reference current dataframe with .",
    "section": "",
    "text": "In tidyverse you can use . to reference the current dataframe. This is really useful for plotting. For example, I’d like to plot channels In7 and In12 in that order but by default In12 will come first. We could save an intermediate dataframe and re-level the factors, but by referencing the dataframe piped into mutate we can skip this step.\ntemp %&gt;% # temp is a down-sampled ephys recording\n  ungroup() %&gt;% \n  gather(key, value, c(\"In7\", \"In12\")) %&gt;% \n  mutate(key = factor(.$key, levels = c(\"In7\", \"In12\"))) %&gt;%      # Note that if you have groupings, you'll need to get rid of them or supply a column of the same length as the group. \n  ggplot(aes(Time, value, color = key, group = interaction(key, Sweep)))+\n  geom_path()\n\n\n\nimage (11).png"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200401_tips_load_r_functions/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200401_tips_load_r_functions/index.html",
    "title": "Loading R functions from source",
    "section": "",
    "text": "You can keep your working Rmd easier to navigate and less buggy by 1) packaging code into functions and 2) adding them to a companion Rfile. Load your functions with source() in the same block you load your libraries with a relative path, full path, or ideally with here().\nlibrary(here)\nsource(here(\"R\", \"02MoniterGapJunction.R\")) #here's output is effectively ../R/02MoniterGapJunction.R"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200515_tips_git_gitignore/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200515_tips_git_gitignore/index.html",
    "title": "Don’t Ignore Your .gitignore",
    "section": "",
    "text": "If you’re using git for version control, don’t forget about the .gitignore file. Anything large and static (like .abfs) or procedurally generated (e.g. plots) you can toss in the .gitignore and you’ll not see it when you commit.\nI have tabular data that lives in ./inst/extdata/ is processed by a script and then saved as a .rds in ./data/. Here’s my .gitignore.\n.Rproj.user\n.Rhistory\n.RData\n.Ruserdata\n# Don't track ABFs -- large and static\n*.abf\n# Don't track files that are generated from the scripts\n/data/*\nFor more check out git-scm"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200625_tips_r_profiling_with_tictoc/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200625_tips_r_profiling_with_tictoc/index.html",
    "title": "Informal profiling with tictoc",
    "section": "",
    "text": "The tictoc library has convenience functions for timing code. Here’s the basic usage relative to timing with base R.\nlibrary(tictoc)\ntic()\n# code here\ntoc()\n\ntic &lt;- Sys.time()\n# code here\ntoc &lt;- Sys.time()\nprint(toc - tic)\nWhere this library excels is when you want to time multiple parts of your code. Each tic pushes the time onto a stack and each toc pops the most recent time from said stack. That means you don’t have to worry about assigning several timing variables even if you want to time nested code.\ntic()\n# stack is 1 deep\nfor (i in 1:10) {\n     tic()\n     # stack is now 2 deep\n     for (j in 1:10){\n          tic()\n          # stack is now 3 deep\n          toc()\n     }\n     toc()\n}\ntoc()"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/201218_r_install_local_post_upgrade/index.html",
    "href": "posts/DanielKick/210000_phd_tips/201218_r_install_local_post_upgrade/index.html",
    "title": "Installing Local Libraries Post Update",
    "section": "",
    "text": "I just ran into the same issue after updating R on OSX — none of the libraries are associated with the new version. I tested and abandoned directly copying the libraries over in favor programmatic reinstallation. The downside to this is that it’s slower. The upside is that if you just copy them, R will ask you to update the packages anyway.\nThis won’t work for libraries that aren’t on CRAN but drastically reduces the number of libraries you’re installing by hand. In my case this took care of all but about 3% of the libraries I had installed for 3.6.\nall_packages &lt;- list.files(\"/Library/Frameworks/R.framework/Versions/3.6/Resources/library/\")\ninstalled_packages &lt;- list.files(\"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/\")\nall_packages &lt;- all_packages[!(all_packages %in% installed_packages)]\n\noptions(install.packages.compile.from.source = \"always\")\n\nfor (package in all_packages){\n  try(install.packages(package))\n}\n\noptions(install.packages.compile.from.source = \"interactive\")"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200604_visualize_outlier_influence/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200604_visualize_outlier_influence/index.html",
    "title": "Visualize the Influence of Outliers",
    "section": "",
    "text": "If you’re looking at linear relationships, give this code a spin. It’ll automatically flag outliers based on 1.5*IQR and show you the fit with and without the outliers. Since the output is a ggplot, you and add to it or tweak the aesthetics of the output (see example below).\nHere’s the function.\nscatter_w_wo_outliers &lt;- function(temp = filter(M, Time == \"Baseline\"),\n                                  X = \"bkkca\", \n                                  Y = \"Ihtk.0\"){\n  \n  # flag outliers based on 1.5xIQR from median\n  X_low  &lt;- median(temp[[X]], na.rm = T) - (1.5 * IQR(temp[[X]], na.rm = T))\n  X_high &lt;- median(temp[[X]], na.rm = T) + (1.5 * IQR(temp[[X]], na.rm = T))\n  Y_low  &lt;- median(temp[[Y]], na.rm = T) - (1.5 * IQR(temp[[Y]], na.rm = T))\n  Y_high &lt;- median(temp[[Y]], na.rm = T) + (1.5 * IQR(temp[[Y]], na.rm = T))\n  \n  X_pass &lt;- (temp[[X]] &gt; X_low) * (temp[[X]] &lt; X_high)\n  Y_pass &lt;- (temp[[Y]] &gt; Y_low) * (temp[[Y]] &lt; Y_high)\n\n  temp$flag &lt;- ifelse((X_pass * Y_pass) == 1, T, F)\n  \n  \n  # Duplicate so we have dataset 1, 2 (introduces duplicates)\n  temp &lt;- rbind(temp[temp$flag == T, ], mutate(temp, flag = F))\n  \n  formula1 &lt;- y ~ x\n  \n  plt &lt;- ggplot(temp, aes_string(X, Y, color = \"flag\"))+\n    geom_smooth(data = temp, method = lm, se = F, fullrange = T)+\n    geom_point(data = temp)+\n    geom_point(data = temp, color = \"black\", shape = 1)+\n    geom_point(data = temp[temp$flag, ])+\n    ggpmisc::stat_poly_eq(aes(label =  paste(stat(eq.label), \"*\" with \"*\", \n                                             stat(rr.label), \"*\", \"*\", \n                                             stat(f.value.label), \"*\", and \"*\",\n                                             stat(p.value.label), \"*\".\"\",\n                                             sep = \"\")),\n                          formula = formula1, parse = TRUE, size = 4)+\n    \n    scale_color_manual(values = c(\"darkgray\", \"black\"))+\n    theme_bw()+\n    theme(legend.position = \"\")\n  \n  return(plt)\n}\nHere’s a reproducible example. We’re creating a Simpson’s paradox by giving the “outliers” a negative slope and the real data a positive slope. I’ve added a red line showing the true relationship.\nset.seed(45645684)\ndf &lt;- data.frame(x = rnorm(30, mean = 10, sd = 4),\n                 noise = runif(30, min = -2, max = 2),\n                 y = NA,\n                 is_outlier = rbinom(30, 1, prob = 0.2))\n\n\ndf$y &lt;- ifelse(df$is_outlier, \n               -5*df$x+df$noise,\n               2*df$x+df$noise)\n\nscatter_w_wo_outliers(temp = df,\n                      X = \"x\",\n                      Y = \"y\")+\n  geom_abline(slope = 2, intercept = 0, color = \"firebrick\")\n\n\n\nimage (7).png\n\n\n2020-6-4 Daniel Here’s a related utility function. For a given column it’ll return a logical vector where outliers are FALSE.\nw_in_x_iqr &lt;- function(col_in, multiplier = 1.5){\n  col_in &lt;- as.vector(col_in)\n  \n  X_low  &lt;- median(col_in, na.rm = T) - (multiplier * IQR(col_in, na.rm = T))\n  X_high &lt;- median(col_in, na.rm = T) + (multiplier * IQR(col_in, na.rm = T))\n  X_pass &lt;- (col_in &gt; X_low) * (col_in &lt; X_high)\n  \n  return(as.logical(X_pass))\n}\ne.g.\n&gt; mutate(M, ex = w_in_x_iqr(bkkca)) %&gt;% select(bkkca, ex) %&gt;% arrange(bkkca) %&gt;% tail()\n\n#  A tibble: 6 x 2\n#   bkkca ex   \n#   &lt;dbl&gt; &lt;lgl&gt;\n# 1 3056. TRUE \n# 2 3222. TRUE \n# 3 3255. TRUE   # Within bounds\n# 4 3552. FALSE  # Outside bounds \n# 5 3817. FALSE\n# 6 6740. FALSE"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/230523_r_a_comment_is_a_comment/index.html",
    "href": "posts/DanielKick/210000_phd_tips/230523_r_a_comment_is_a_comment/index.html",
    "title": "A Comment is a Comment.",
    "section": "",
    "text": "R treats “#” as a comment in text files.\nIf you have a file with this in a a header field (e.g. for biologists “# chromosomes”) R will fail to load the file. The solution is to pass in an explicit comment character like so read.table(“table_file.txt”, comment.char=‘’)."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/240321_r_go_read_about_function_call/index.html",
    "href": "posts/DanielKick/210000_phd_tips/240321_r_go_read_about_function_call/index.html",
    "title": "Go Read About R’s Function Call Semantics",
    "section": "",
    "text": "This is the sort of thing you don’t realize until it would be really useful to access the name of a variable or run text as if it were code. I think the most accessible example of R’s wizardry is in plotting- you pass variables (time, mv) to plot or ggplot instead of strings (“time”, “mv”) and magically you get axis labels. R gets access to the value of a variable and its name and no one notices because it just works."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200511_tidyverse_at_variants/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200511_tidyverse_at_variants/index.html",
    "title": "Tidyverse ’_at’ variants",
    "section": "",
    "text": "tldr: tidyverse function variants take slightly different input. Testing out a variant or two can save you a lot of debugging time.\nIn tidyverse watch out for inconsistencies in function versions. There are variants of common functions (e.g. mutate(), mutate_all(), mutate_at()) don’t necessarily behave the same way (or how you would expect).\nHere, I was applying an operation to a grouped df where each Experiment contains several FileNames with multiple observations in each. To keep everything reusable I’m using exp instead of Experiment to select the right col.\n&gt; exp = \"Experiment\"\n&gt; rec = \"FileName\"\n\n&gt; df %&gt;%\n+     dplyr::select(\n+       exp, rec, r11, r12\n+     )\n# A tibble: 564 x 4\n   Experiment FileName           r11   r12\n   &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;\n 1 190808a    190808a_0020.abf  5.06 0.150\n 2 190808a    190808a_0020.abf  5.13 0.160\n 3 190808a    190808a_0020.abf  5.11 0.122\n 4 190808a    190808a_0020.abf  2.49 0.152\n 5 190808a    190808a_0020.abf  2.49 0.195\n# ... with 559 more rows\nAs soon as we do the same thing with group_by() we don’t get the right column even though select() didn’t have an issue with exp.\n&gt; df %&gt;%\n+     dplyr::select(\n+       exp, rec, r11, r12\n+     ) %&gt;%\n+     group_by(exp, rec)\nError: Column `exp` is unknown\nSo we can try explicitly selecting the columns we want as groupings.\n&gt;     df %&gt;%\n+     dplyr::select(\n+       exp, rec, r11, r12\n+     ) %&gt;%\n+     group_by(vars(exp, rec))\nError: Column `vars(exp, rec)` must be length 564 (the number of rows) or one, not 2\nNo dice there. vars() is designed to work with the _at variants so we can try that. et voilà!\n&gt; df %&gt;%\n+     dplyr::select(\n+       exp, rec, r11, r12\n+     ) %&gt;%\n+     group_by_at(vars(exp, rec))\n\n# A tibble: 564 x 4\n# Groups:   Experiment, FileName [69]\n   Experiment FileName           r11   r12\n   &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;\n 1 190808a    190808a_0020.abf  5.06 0.150\n 2 190808a    190808a_0020.abf  5.13 0.160\n 3 190808a    190808a_0020.abf  5.11 0.122\n 4 190808a    190808a_0020.abf  2.49 0.152\n 5 190808a    190808a_0020.abf  2.49 0.195\n# ... with 559 more rows"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200526_tips_update_word_doc_by_links/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200526_tips_update_word_doc_by_links/index.html",
    "title": "Updating Results in Word from Table",
    "section": "",
    "text": "Handy trick: Write your test results into a summary csv. Then while writing you can insert a link to the cell value for a result into your .doc. After that, changing post hoc corrections, re-sampling iterations, or data QC just requires you to re-run your code and let word refresh all the links.\nYou can also do this from excel, but you’ll have to update the formula for the cell used instead.\n/"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/210323_simulation_for_intuition/index.html",
    "href": "posts/DanielKick/210000_phd_tips/210323_simulation_for_intuition/index.html",
    "title": "Using Simulations to Check Your Statistical Intuition",
    "section": "",
    "text": "R’s distribution simulation functions (e.g. dbinom, runif) make it quick and easy to double check one’s intuitions. For example, I’d been thinking that under H0 the distribution of correlations from normal samples should drop off sharply as you go away from 0 such that a shift in correlation from 0 -&gt; 0.1 is much more likely than 0.8 -&gt; 0.9.\nSo I used purrr::map() to run a quick simulation. Here we simulate the null distribution based on 100,000 observations and compute the chance of a value being above 0.7. If it was uniform we would expect ~15% (.03/2) of the distribution to be here but end up with ~1.2% with the drop off.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nset.seed(89745) \ncor_check &lt;- map(1:100000, function(i){ \n  cor(rnorm(10), rnorm(10), method = \"pearson\") \n}) \ncor_check &lt;- data.frame(cor = do.call(rbind, cor_check))\n\nmean(cor_check$cor &gt;= 0.7)*100 \n\n[1] 1.227\n\n#1.227 Percent \n\n\nggplot(cor_check, aes(x = cor))+\n  geom_histogram(binwidth = 0.05)"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200822_r_datatype_gganimate/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200822_r_datatype_gganimate/index.html",
    "title": "Data Type Matters In gganimate",
    "section": "",
    "text": "Be mindful of your data types. Sometimes T == 1 == 1.0 (logical, int, double) but assuming these are equivalent can get you into trouble. For example, in these animations, the only difference is the data is coerced to logicals or integers."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/210309_ggplot_expressions/index.html",
    "href": "posts/DanielKick/210000_phd_tips/210309_ggplot_expressions/index.html",
    "title": "ggplot: Parsing Expressions",
    "section": "",
    "text": "A useful trick is to pass expressions into ggplot. Here I’ve used the following as arguments in labs().\nc(\"r11\", \"r1\", \"Ihtk.0\", \"Ihtk.Slope\", \"Ia.0\", \"Ia.Slope\", \"vrest\")\nc(expression(M\\~Omega), expression(M\\~Omega), \"nA\", expression(frac(nA,\nmV)), \"nA\", expression(frac(nA, mV)), \"mV\" )\n\n\n\nimage (30).png\n\n\nYou can also do something like this theme(plot.title = element_text(face=\"italic\")) to add italics to the figure title."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/210221_dendrograms/index.html",
    "href": "posts/DanielKick/210000_phd_tips/210221_dendrograms/index.html",
    "title": "Workaround for Plotting Dendrograms",
    "section": "",
    "text": "I think I have a solution to get decent enough dendrograms without fussing with base graphics.\nThe overview of my workaround is to cluster with pvclust, extract $hclust , plot it as a dendrogram, coerce into a ggplot. This makes it easy enough to replicate the functionality of the colored_bars() function by making additional plots. The function below makes a few plots in addition to the dendrogram. If you end up working with base graphics anyway, dendextend is still worth a look.\nHere’s an example:\n# needed \nlibrary(pvclust)\nlibrary(tidyverse)\nlibrary(dendextend) # for color_labels\nlibrary(ggnewscale) # to accommodate two fill scales  https://github.com/eliocamp/ggnewscale\n\n# recommended\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(patchwork)\nlibrary(scales) # for overiding scientific notation on dendrogram y axis \n\n# Make a demo dataset\ntux &lt;- select(palmerpenguins::penguins, \n              species,\n              bill_length_mm, bill_depth_mm, \n              flipper_length_mm, body_mass_g) \n\ntux &lt;- tux[complete.cases(tux), ]\n\nset.seed(54646)\ntux &lt;- tux[sample(1:nrow(tux), 30), ] # for faster demo clustering\n\n# Example use\no &lt;- \n  mk_hclust_plts(\n    df = mutate(tux, uid = paste(seq(1, nrow(tux)), species, sep = \"-\")),\n    cluster_by = c(\"bill_length_mm\", \"bill_depth_mm\", \n                   \"flipper_length_mm\", \"body_mass_g\"),\n    uid_col = \"uid\",\n    n_clusters = 3,\n    true_groups = \"species\",\n    true_colors = RColorBrewer::brewer.pal(3, \"Set2\"),\n    cluster_colors = RColorBrewer::brewer.pal(3, \"Set1\") \n  )\n\n\n# Patchwork to arrange the output plots\n(o$dendrogram_both+\n    scale_y_continuous(limits = c(-.0001, 0.00022), labels = scales::comma)\n)/ \n  (o$group_compare_tile+\n     # lims(y =  c(2, -2))+ # y axis can be flipped like so\n     theme(legend.position = \"\")\n  ) / \n  (o$heatmap_raw + theme(legend.position = \"right\")) / \n  (o$heatmap_z + theme(legend.position = \"right\")) + \n  patchwork::plot_layout(heights = c(5, .3, 1.25, 1.25))\n\n\n# example 2\n\n# o &lt;- \n# mk_hclust_plts(\n#   df = mutate(iris, uid = paste(seq(1, nrow(iris)), Species, sep = \"-\")),\n#   cluster_by = c(\"Sepal.Length\", \"Sepal.Width\", \"Petal.Length\", \"Petal.Width\"),\n#   uid_col = \"uid\",\n#   n_clusters = 3,\n#   true_groups = \"Species\",\n#   true_colors = RColorBrewer::brewer.pal(3, \"Set2\"),\n#   cluster_colors = RColorBrewer::brewer.pal(3, \"Set1\") \n# )\nps, it’s worth checking your code on sample datasets (penguins,iris, mpg, etc. ). That’ll help iron out weird behavior sooner rather than later. \n(2021-2-21) Edit: The original function here depended on the factor levels of the clusters and true groups to make the color’s consistent between plots (e.g. factors ordered abcdABCD not aAbBcCdD). This breaks down with some cases (e.g. if you start group names with a number (e.g. 0hours)). The below edit uses ggnewscale to fix this.\nmk_hclust_plts &lt;- function(\n  df = unite(M_winxiqr, uid, Experiment, Cell, sep = \"-\"),\n  cluster_by = c(\"vrest\", \"r11\", \"r1\", \"Ihtk.0\", \"Ihtk.Slope\", \"Ia.0\", \"Ia.Slope\"),\n  uid_col = \"uid\",\n  n_clusters = 3,\n  true_groups = \"Condition\",\n  true_colors = RColorBrewer::brewer.pal(3, \"Set2\"),\n  cluster_colors = RColorBrewer::brewer.pal(3, \"Set1\")\n){\n  df &lt;- as.data.frame(df) \n  \n  if (!exists(\"cluster_colors\")){\n    cluster_colors = rainbow(n_clusters)\n  }\n  ## prep\n  # move uid to rowname\n  row.names(df) &lt;- df[[uid_col]]\n  df_groups &lt;- select(df, all_of(true_groups))\n  df &lt;- df[, cluster_by]\n  \n  ## Cluster\n  cluster &lt;- pvclust(t(df),\n                     method.hclust = \"ward.D2\",\n                     method.dist = \"correlation\",\n                     use.cor = \"pairwise.complete.obs\")\n  \n  \n  ## make dendrogram  ####\n  dend &lt;- cluster$hclust %&gt;% \n    as.dendrogram() \n  \n  # iteratively coloring the labels is a workaround to get the \"true\" groups shown\n  dend_labs &lt;- rownames_to_column(df_groups, var = \"rownames\")[, ]\n  dend_labs &lt;- full_join(data.frame(rownames = labels(dend)), dend_labs)\n  for(i in seq_along(unique(df_groups[[true_groups]]))){\n    true_group &lt;- unique(df_groups[[true_groups]])[i]\n    true_color &lt;- true_colors[i]\n    \n    dend &lt;- dend %&gt;% \n      dendextend::color_labels(\n        col = true_color, \n        labels = dend_labs[dend_labs[[true_groups]] == true_group, \"rownames\"]) \n    \n    \n  }\n  \n  dend &lt;- dend %&gt;% \n    set(\"branches_k_color\", \n        k = n_clusters, \n        value = cluster_colors\n    ) %&gt;% \n    set(\"branches_lwd\", 0.7) %&gt;%\n    set(\"labels_cex\", 0.6) \n\n  dend_cluster_only &lt;- dend %&gt;% \n    set(\"labels_colors\",\n        k = n_clusters,\n        value = cluster_colors) %&gt;%\n    as.ggdend()\n  \n  \n  dend &lt;- dend %&gt;% \n    as.ggdend()\n  \n  \n  \n  plt_dend_cluster_only &lt;- ggplot(dend_cluster_only)+\n    theme(axis.ticks.y = element_line(),\n          axis.text.y = element_text(),\n          axis.line.y = element_line())\n  \n  \n  plt_dend &lt;- ggplot(dend)+\n    theme(axis.ticks.y = element_line(),\n          axis.text.y = element_text(),\n          axis.line.y = element_line())  \n  \n\n  ## Add reality ribbon with or without clustering result ####\n  groups_to_plt &lt;- full_join(\n    as.data.frame(dend$labels),\n    rownames_to_column(var = \"label\", df_groups))\n  \n  plt_grouping &lt;- groups_to_plt %&gt;% \n    ggplot(aes_string(x=\"x\", y=\"0\", fill = true_groups))+\n    geom_tile()+\n    scale_fill_manual(values = true_colors)+\n    theme_void()+\n    labs(x = \"\", y = \"\")+\n    theme(legend.position = \"left\")\n  \n  plt_grouping_contrast &lt;- ggplot()+\n    geom_tile(data = groups_to_plt, aes_string(x=\"x\", y=\"0.5\", fill = true_groups))+\n    scale_fill_manual(values = true_colors)+\n    \n    ggnewscale::new_scale(\"fill\") +\n    geom_tile(data = data.frame(x = seq_along(dend_cluster_only$labels$col),\n                                cluster_groups = as.character(as.numeric(as.factor(dend_cluster_only$labels$col)))\n    ),\n    aes_string(x=\"x\", y= \"-0.5\", fill = \"cluster_groups\"),\n    )+\n    scale_fill_manual(values = cluster_colors)+\n    \n    theme_void()+\n    labs(x = \"\", y = \"\")+\n    theme(legend.position = \"left\")\n  \n  \n  ## Add heatmap  ####\n  data_to_plt &lt;- full_join(\n    as.data.frame(dend$labels),\n    rownames_to_column(var = \"label\", df)) \n  \n  data_to_plt &lt;- \n    data_to_plt %&gt;% \n    gather(\"key\", \"value\", \n           names(data_to_plt)[\n             !(names(data_to_plt) %in% c(\"x\", \"y\", \n                                         \"label\", \"col\", \"cex\", \n                                         true_groups))\n           ])\n  \n  plt_heatmap_raw &lt;- data_to_plt %&gt;% \n    ggplot(aes(x, \n               y = key, \n               fill = value))+\n    geom_tile()+\n    scale_fill_viridis_c()+\n    labs(x = \"\", y = \"\")+\n    theme(panel.background = element_blank(),\n          axis.ticks.x = element_blank(),\n          axis.text.x = element_blank(),\n          legend.position = \"left\")\n  \n  \n  plt_heatmap_z &lt;- data_to_plt %&gt;% \n    group_by(key) %&gt;% \n    mutate(mean = mean(value, na.rm = T),\n           sd = sd(value, na.rm = T)) %&gt;% \n    mutate(value = ((value - mean)/sd)) %&gt;% # Now Z scores\n    ggplot(aes(x, \n               y = key, \n               fill = value))+\n    geom_tile()+\n    scale_fill_viridis_c()+\n    labs(x = \"\", y = \"\")+\n    theme(panel.background = element_blank(),\n          axis.ticks.x = element_blank(),\n          axis.text.x = element_blank(),\n          legend.position = \"left\")\n  \n  \n  ## Return plots, manually tweak layout  ####\n  return(\n    list(\n      pvclust_out = cluster,\n      dendrogram_clusters = plt_dend_cluster_only,\n      dendrogram_both = plt_dend,\n      group_tile = plt_grouping,\n      group_compare_tile = plt_grouping_contrast,\n      heatmap_raw = plt_heatmap_raw,\n      heatmap_z = plt_heatmap_z\n    )\n  )\n}"
  },
  {
    "objectID": "posts/DanielKick/210713_python_custom_functions/index.html",
    "href": "posts/DanielKick/210713_python_custom_functions/index.html",
    "title": "Tips: Reusing Custom Functions",
    "section": "",
    "text": "Amendment: For packaging functions also see nbdev.\nI wanted to reuse a custom function across a few scripts without having copies of the same code in each script. The solution I found is to set up a module to hold these functions. This seems straightforward once you know how it’s done.\n\nSet up a directory containing your functions and a blank file called __init__.py.\n\nAdd the directory containing your module directory to the system path (here MaizeModel instead of MaizeModel\\\\library). If you’re on OSX or linux you’ll probably use single forward slashes instead of double backslashes.\n\nFinally import and call your functions.\n\nCaveats:\n\nIt seems that the system path isn’t permanently altered by sys.path.append, so one would need that at the start of the script or modify it some other way.\nIf your custom functions are in the in the same directory as your script, I think you can skip all of this and just import them.\nIf your functions are in a sub-directory of the same directory as your script, I think you can get away without adding the directory to the path."
  },
  {
    "objectID": "posts/DanielKick/240514_bash_jupyter_nb/index.html",
    "href": "posts/DanielKick/240514_bash_jupyter_nb/index.html",
    "title": "Quick Tip: Write notebooks, run scripts",
    "section": "",
    "text": "Much of what I write is in notebooks (thanks to the lovely nbdev library) but there are times where this is not convenient. For instance, while tuning hyperparameters or running other processes that can take a long time, it would be useful to detach a notebook from my IDE while it’s running. One dead simple way to do this on linux is with the “no hangup” command (nohup).\nAll we have to do is: 1. activate the enviroment:\n$ conda activate my_env\n\nuse jupyter to create a python script from the desired notebook: (my_env) $ jupyter nbconvert --to python my_notebook.ipynb\nrun the notebook (in our environment) with nohup so that the shell can be disconnected and & to run the command in the background (my_env) $ nohup python ./my_notebook &\n\nEt voilà! The process shows up on the GPU and we don’t have to worry about a bad internet connection or anything else stopping it before it’s finished.\n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A      1461      G   /usr/lib/xorg/Xorg                            4MiB |\n|    0   N/A  N/A    767952      C   python                                     3764MiB |\n+---------------------------------------------------------------------------------------+"
  },
  {
    "objectID": "posts/DanielKick/220216_python_silent_replace/index.html",
    "href": "posts/DanielKick/220216_python_silent_replace/index.html",
    "title": "Tips: For those coming from R: Silent In Place Replacement",
    "section": "",
    "text": "Silent, in place assignment updating an object This tripped me up even though it’s consistent with how I’ve seen other objects behave. I needed an attribute to hold data extracted from a collection of files in a directory and created a class for this.\nclass hps_search_experiment:\n    def __init__(self, path=\"\", trial_type=''):\n        self.path = path\n        self.trial_type = trial_type\n        self.hps_best_trial = None\n        \n    def process_hps_files(self):\n        # ...\n        \n        self.hps_best_trial = hps_best_trial\nHowever, running like so fails.\ntest = hps_search_experiment(\n    path = './hps_search_intermediates_G/', \n    trial_type = 'rnr')\n    \ntest = test.process_hps_files()\ntest.hps_best_trial\n\n#&gt; AttributeError: 'NoneType' object has no attribute 'hps_best_trial'\nThis had me baffeld because I was thinking with R’s norms of data &lt;- data %&gt;% funciton() where in place replacement is the exception. Instead I needed to be thinking with python’s base object norms (e.g. a_list.extend(['b', 'c']) ). This fails because I overwrote test with the output of the method, which returns notthing since it’s overwriting attributes within test’s scope.\nThese would also work to update the attribute:\nself.hps_best_trial = hps_best_trial\n\nhps_search_experiment.__setattr__(self, \"hps_best_trial\", hps_best_trial)\n\n# if it's initialized as a list\nself.hps_best_trial.append([hps_best_trial]) \n# if a dict is initialized for data\nself.data = {'a':1}\nself.data.update({'hps_best_trial':hps_best_trial})"
  },
  {
    "objectID": "posts/manuscript-kick-et-al-2023/index.html",
    "href": "posts/manuscript-kick-et-al-2023/index.html",
    "title": "Yield prediction through integration of genetic, environment, and management data through deep learning",
    "section": "",
    "text": "https://academic.oup.com/g3journal/article/13/4/jkad006/6982634"
  },
  {
    "objectID": "posts/manuscript-willenbrink-et-al-2016/index.html",
    "href": "posts/manuscript-willenbrink-et-al-2016/index.html",
    "title": "The Hillary Climber trumps manual testing: an automatic system for studying Drosophila climbing",
    "section": "",
    "text": "https://www.tandfonline.com/doi/abs/10.1080/01677063.2016.1255211"
  },
  {
    "objectID": "posts/manuscript-kick-and-washburn-2023/index.html",
    "href": "posts/manuscript-kick-and-washburn-2023/index.html",
    "title": "Ensemble of BLUP, Machine Learning, and Deep Learning Models Predict Maize Yield Better Than Each Model Alone",
    "section": "",
    "text": "https://www.biorxiv.org/content/10.1101/2023.03.30.532932v1"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "notes",
    "section": "",
    "text": "\\(\\color{red}{\\text{TODO:}}\\) Add the following to `_quarto.yml`\n\nRe-list Projects\n\n\nRestore search\nMaybe restore title\n\n\n# website:\n#   title: \"Portfolio\"\n#   \n#   search: \n#     location: navbar\n#     type: overlay\n#   navbar:\n#     left: \n#       - text: Home\n#         href: index.qmd\n#       - text: Projects\n#         href: projects.qmd\n#     right:\n#       - text: Resume\n#         href: resume.qmd\n#       - text: CV\n#         href: curriculum-vitae.qmd\n#       - icon: github\n#         href: https://github.com/danielkick\n#       - icon: linkedin\n#         href: https://www.linkedin.com/in/daniel-kick-5a449b9a/\n\nDirectly usable icons can be found at:\nhttps://icons.getbootstrap.com/\nIf you use gifs gzip to decrease size to prevent the website from becoming sluggish\ncode highlighting can be found at https://quarto.org/docs/output-formats/html-code.html#highlighting"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "If no file appears, please follow this link to GitHub."
  },
  {
    "objectID": "posts/manuscript-kick-and-schulz-2022/index.html",
    "href": "posts/manuscript-kick-and-schulz-2022/index.html",
    "title": "Timing-Dependent Potentiation and Depression of Electrical Synapses Contribute to Network Stability in the Crustacean Cardiac Ganglion",
    "section": "",
    "text": "https://www.jneurosci.org/content/42/35/6751"
  },
  {
    "objectID": "posts/manuscript-northcutt-kick-et-al-2019/index.html",
    "href": "posts/manuscript-northcutt-kick-et-al-2019/index.html",
    "title": "Molecular profiling of single neurons of known identity in two ganglia from the crab Cancer borealis",
    "section": "",
    "text": "https://www.pnas.org/doi/abs/10.1073/pnas.1911413116"
  },
  {
    "objectID": "posts/manuscript-kick-and-schulz-2019/index.html",
    "href": "posts/manuscript-kick-and-schulz-2019/index.html",
    "title": "Cell Communication: Studying gap junctions with PARIS",
    "section": "",
    "text": "https://elifesciences.org/articles/45207"
  },
  {
    "objectID": "posts/DanielKick/240624_r_GAPIT_container/index.html",
    "href": "posts/DanielKick/240624_r_GAPIT_container/index.html",
    "title": "Building a GWAS Container",
    "section": "",
    "text": "Today we’re going to look at building a container to run a GWAS1 on a computing cluster. Container construction can be rigorous and necessitate a fair bit of understanding but we can get a lot of the benefits without, say, doing everything we can to make the container small enough to run on the idea of a computer.\nWe’ll start by looking at a simple script to run GWAS with GAPIT. We need only install tidyverse and GAPIT so our container will be simple.\nInstead of Docker we’re going to use singularity (aka apptainer) to avoid needing root access – which we won’t have on an HPC. We begin by creating a definition file with installation instructions for our analysis’ dependencies. Happily, we can build off the Docker ecosystem which lets us avoid a lot of steps by simply using an image from the rocker project with R installed. Additional dependencies are installed as if we were working on the linux command line2.\nNext we build the container. If you have root access the first command will work. Otherwise try the second. The machine that builds the container will need internet access. I’d recommend using a local machine to build this (if you’re on windows, take a look into the Windows Subsystem for Linux ). Depending on the HPC you’re using there might be a node set aside on the HPC for this (using the login nodes for this sort of thing is not recommended – it can interfere with other people accessing it).\nAfter you’ve moved your files and the container to the HPC you can open a shell in the container. This shell should still have access to the host’s files (but here’s a link to how to bind baths in case it doesn’t). From there you can start R in interactive mode or launch the R script."
  },
  {
    "objectID": "posts/DanielKick/240624_r_GAPIT_container/index.html#footnotes",
    "href": "posts/DanielKick/240624_r_GAPIT_container/index.html#footnotes",
    "title": "Building a GWAS Container",
    "section": "Footnotes",
    "text": "Footnotes\n\n\na Genome Wide Association Study identifies regions in a genome that are associated with a trait↩︎\nWhile beyond the scope of this post, you can also create sandbox images which exist as a directory instead of .sif file. Running a such an image in writable mode allows for interactive tinkering.↩︎"
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html",
    "href": "posts/DanielKick/230915_vnn_overview/index.html",
    "title": "Making a “Visible” Neural Network",
    "section": "",
    "text": "In most neural networks, neurons are not parametric in the same way that linear models are. In a image recognition model there may be neuron which functions to detects edges but when the model is set up initially one can’t point to a neuron and say what it will do or represent. This can make interpreting the weights in a model tricky.\nVisible neural networks (VNN) are one way to get around this problem by making the structure of a model reflect the process being modeled. In a VNN, influential sub-components may be interpreted as implicating the process they represent as being important. Within biology VNNs have been used by Ma et al. 2018 and Hilten et al. 2021 working in yeast and humans respectively (in the later mixed performance, seemingly based on trait complexity)."
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#whats-a-visual-neural-network",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#whats-a-visual-neural-network",
    "title": "Making a “Visible” Neural Network",
    "section": "",
    "text": "In most neural networks, neurons are not parametric in the same way that linear models are. In a image recognition model there may be neuron which functions to detects edges but when the model is set up initially one can’t point to a neuron and say what it will do or represent. This can make interpreting the weights in a model tricky.\nVisible neural networks (VNN) are one way to get around this problem by making the structure of a model reflect the process being modeled. In a VNN, influential sub-components may be interpreted as implicating the process they represent as being important. Within biology VNNs have been used by Ma et al. 2018 and Hilten et al. 2021 working in yeast and humans respectively (in the later mixed performance, seemingly based on trait complexity)."
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#a-hypothetical-gene-network",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#a-hypothetical-gene-network",
    "title": "Making a “Visible” Neural Network",
    "section": "A hypothetical gene network",
    "text": "A hypothetical gene network\nBefore scaling to representing gene networks, I built a simple test case and will walk through it below, with all the necessary code (but some of it hidden1 for brevity).\n\nHere we have a hypothetical network which involves two genes (a1_input, a2_input), variants of which affect some initial processes (b1, b2), which in turn affect a second set of processes (c1, c2). I’ll use these last processes to predict my trait of interest (y_hat).\nThis is a directed acyclic graph, meaning that processes have an order (the arrows) and there are no loops (c1 doesn’t some how change a1_input). The model I’d like to end up with is a neural network with a structure that mirrors this graph 2 with each node representing one or more layers of neurons.\nBeginning with the end in mind, I need a way to specify: 1. The data the graph operates on 1. The process graph and each node’s attributes 1. How to “move” through the graph"
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#the-data-itself",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#the-data-itself",
    "title": "Making a “Visible” Neural Network",
    "section": "1. The data itself",
    "text": "1. The data itself\nMy example trait, \\(y\\) is either 0 or 1 (plus a little noise). It’s controlled by two genes which are represented as tensor3 containing values for each possible nucleotide (ACGT) for each SNP measured in the gene. Conveniently, both genes either contain all 0’s or all 1’s and when there are only 0’s \\(y\\) will be around 0 (and the same for 1).\nThis of course means that in this population no nucleotides (all 0s) were observed or all nucleotides (all 1s) were simultaneously observed. Don’t ask me how this is possible 🤔. For real data these values would be probability of seeing a given nucelotide so “A” might be [1, 0, 0, 0]4.\n\nn_obs = 100 # 100 obs for each group\ny_true = torch.from_numpy(np.concatenate([\n        np.zeros((n_obs, )),\n        np.ones( (n_obs, ))], 0)) + .1* torch.rand(2*n_obs,)\n        \ninput_tensor_dict = {\n    'a1_input': torch.from_numpy(np.concatenate([\n        np.zeros((n_obs, 4, 3)),\n        np.ones( (n_obs, 4, 3))], 0)),\n    'a2_input': torch.from_numpy(np.concatenate([\n        np.zeros((n_obs, 4, 2)),  \n        np.ones( (n_obs, 4, 2))], 0))}\n\nx_list_temp = [input_tensor_dict[key].to(torch.float) for key in input_tensor_dict.keys()]\nx_list_temp\n# output\n                        # Probability of\n[tensor([[[0., 0., 0.], # A\n          [0., 0., 0.], # C\n          [0., 0., 0.], # G\n          [0., 0., 0.]],# T\n\n         ...,\n\n         [[1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.]]]),\n\n tensor([[[0., 0.],\n          [0., 0.],\n          [0., 0.],\n          [0., 0.]],\n\n         ...,\n\n         [[1., 1.],\n          [1., 1.],\n          [1., 1.],\n          [1., 1.]]])]\n\nThen this data can be packaged nicely in a DataLoader5. This will retrieve the trait (y) and SNPs for each gene (in x_list) for 20 observations at a time.\n\ntraining_dataloader = DataLoader(\n  ListDataset(\n    y = y_true[:, None].to(torch.float32), # Set as 32 bit float to match network\n    x_list = [e.to(torch.float32) for e in x_list_temp]),\n    batch_size = 20,\n    shuffle = True)"
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#defining-the-graph",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#defining-the-graph",
    "title": "Making a “Visible” Neural Network",
    "section": "2. Defining the graph",
    "text": "2. Defining the graph\nThe structure of a graph can be nicely represented as a python dictionary so I’ll begin with that:\n\nnode_connections = {\n  'y_hat':['c1', 'c2'],\n  'c1':['b1'],\n  'c2':['b2'],\n  'b1':['a1_input', 'b2'],\n  'b2':['a2_input'],\n  'a1_input': [],\n  'a2_input': []\n}\n\nEach node will have an input and output size stored in a dictionary. The output sizes are easy, all nodes will have the same size except for the last node, which has predicts y, which will have a size of 1.\n\nnode_list = list(node_connections.keys())\n\ndefault_output_size = 20\noutput_size_dict = dict(zip(node_list, \n                        [default_output_size for i in range(len(node_list))]))\noutput_size_dict['y_hat'] = 1 \noutput_size_dict\n# output\n{'a1_input': 20,\n 'a2_input': 20,\n 'b1': 20,\n 'b2': 20,\n 'c1': 20,\n 'c2': 20,\n 'y_hat': 1}\n\nThe input sizes are a little trickier. A node’s input should be the number of SNPs in a gene (if it’s an input node) or the sum of the outputs of the nodes on which it depends (e.g. y_hat’s input size is the sum of c1 and c2’s outputs). To do this, I’m going to copy the dictionary with all the connections between nodes, then swap the node names for their output sizes. Summing the list of these output values will be the required input size. Data nodes don’t depend on input from other nodes, so those will have an input shape of 0.\n\ninput_size_dict = node_connections.copy()\n\nno_dependants = [e for e in node_connections.keys() if node_connections[e] == []]\n\n# use the expected output sizes from `output_size_dict` to fill in the non-data sizes\ntensor_ndim = len(input_tensor_dict[list(input_tensor_dict.keys())[0]].shape)\nfor e in tqdm(input_size_dict.keys()):\n    # overwrite named connections with the output size of those connections\n    # if the entry is in no_dependants it's data so it's size needs to be grabbed from the input_tensor_dict\n    input_size_dict[e] = [\n        (list(input_tensor_dict[ee].shape)[1]*list(input_tensor_dict[ee].shape)[2]) \n        if ee in no_dependants\n        else output_size_dict[ee] for ee in input_size_dict[e]]\n\n# Now walk over entries and overwrite with the sum of the inputs\nfor e in tqdm(input_size_dict.keys()):\n    input_size_dict[e] = np.sum(input_size_dict[e])\n    \ninput_size_dict\n# output\n{'y_hat': 40,\n 'c1': 20,\n 'c2': 20,\n 'b1': 32,\n 'b2': 8,\n 'a1_input': 0.0,\n 'a2_input': 0.0}\n\nNow we can update the graph from above adding in the input/output sizes."
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#how-to-move-through-the-graph",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#how-to-move-through-the-graph",
    "title": "Making a “Visible” Neural Network",
    "section": "3. How to move through the graph",
    "text": "3. How to move through the graph\nTo calculate the prediction for an observation each node in the graph needs to be run after all it’s input nodes have been run. Specifically, I need a list of nodes, ordered such that each node comes after all the nodes on which it depends.\nThis takes little doing. Here I use some custom helper function to find the unique entries in a dictionary, the “top” nodes (those on which no other nodes depend).\n\n# start by finding the top level -- all those keys which are themselves not values\n# helper function to get all keys and all value from a dict. Useful for when keys don't have unique values.\ndef find_uniq_keys_values(input_dict):\n    all_keys = list(input_dict.keys())\n    all_values = []\n    for e in all_keys:\n        all_values.extend(input_dict[e])\n    all_values = list(set(all_values))\n\n    return({'all_keys': all_keys,\n           'all_values': all_values})\n\n# find the dependencies for run order from many dependencies to none\n# wrapper function to find the nodes that aren't any other nodes dependencies.\ndef find_top_nodes(all_key_value_dict):\n    return([e for e in all_key_value_dict['all_keys'] if e not in all_key_value_dict['all_values']])\n\nSimilar to how I calculated each node’s output size, here I copy the connection dictionary and then manipulate it. I repeatedly identify the top-most nodes in the graph, add them to a list, and then remove them from the dictionary. Repeating this “peels” of the top layer over and over until there are nodes left. The resulting list is ordered from top most to most basal, so reversing it is all that need be done to get the order nodes should be run in.\n\n# find the dependencies for run order from many dependencies to none\ntemp = node_connections.copy()\n\ndependancy_order = []\n# Then iterate\nfor ith in range(100): \n    top_nodes = find_top_nodes(all_key_value_dict = find_uniq_keys_values(input_dict = temp))\n    if top_nodes == []:\n        break\n    else:\n        dependancy_order += top_nodes    \n        # remove nodes from the graph that are at the 'top' level and haven't already been removed\n        for key in [e for e in dependancy_order if e in temp.keys()]:\n             temp.pop(key)\n\n                \n# reverse to get the order that the nodes should be called\ndependancy_order.reverse()                \ndependancy_order\n# output\n['a2_input', 'a1_input', 'b2', 'b1', 'c2', 'c1', 'y_hat']"
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#turn-the-graph-into-a-neural-network",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#turn-the-graph-into-a-neural-network",
    "title": "Making a “Visible” Neural Network",
    "section": "4. Turn the graph into a neural network",
    "text": "4. Turn the graph into a neural network\nSo far, we have the data in a useful format (training_dataloader), a description of what the network should look like (node_connections, input_size_dict, output_size_dict), and the order that nodes in the network should be run in (dependancy_order). With this, we can build the network. I’ll start by defining a node as a linear layer (nn.Linear) that is passed into a ReLU. By creating a function6 for making nodes, changing every node in the network is as easy as editing this function.\n\ndef Linear_block(in_size, out_size, drop_pr):\n    block = nn.Sequential(\n        nn.Linear(in_size, out_size),\n        nn.ReLU())\n    return(block)  \n\nNow, I can go through each node in order of it’s dependencies and have it return the data (if it’s an input node), process inputs with a Linear_block (if it’s not an input node or the output node), or use a linear function to predict the trait7.\n\n# fill in the list in dependency order. \nlayer_list = []\nfor key in dependancy_order:\n    if key in input_tensor_names:\n        layer_list += [\n            nn.Flatten()\n        ]\n    elif key != 'y_hat':\n        layer_list += [\n            Linear_block(in_size=example_dict_input_size[key], \n                         out_size=example_dict_output_size[key])\n                      ]\n    else:\n        layer_list += [\n            nn.Linear(example_dict_input_size[key], \n                      example_dict_output_size[key])\n                      ]"
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#double-checking-the-model-structure",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#double-checking-the-model-structure",
    "title": "Making a “Visible” Neural Network",
    "section": "Double checking the model structure",
    "text": "Double checking the model structure\nUsing the lovely library torchviz, we can visualize every computational step in this model.\n\nThis is a lot to look at, but if we compare it to the earlier graph we can spot the same loop."
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#the-moment-of-truth",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#the-moment-of-truth",
    "title": "Making a “Visible” Neural Network",
    "section": "The moment of truth…",
    "text": "The moment of truth…\nNow all that is left is to see if the model trains. Using the objects describing the graph, the names of the input tensors, and the order nodes should be run it I’ll initialze the network, train it for 200 epochs aaaaaannnnddd….\n\nmodel = NeuralNetwork(example_dict = node_connections, \n                      example_dict_input_size = input_size_dict,\n                      example_dict_output_size = output_size_dict,\n                      input_tensor_names = list(input_tensor_dict.keys()),\n                      dependancy_order = dependancy_order) \n\n\nmodel, loss_df = train_nn_yx(\n    training_dataloader,\n    training_dataloader, # For demo, the training and testing data are the same.\n    model,\n    learning_rate = 1e-3,\n    batch_size = 20,\n    epochs = 200\n)\n\nIt works!\n\nNow all that’s left is to scale it up to a full genome and all the connections between the genes in it 😅."
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#footnotes",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#footnotes",
    "title": "Making a “Visible” Neural Network",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHave a look at the page source.↩︎\nThis will not be a graph neural network, although they may be effective here.↩︎\nA box of numbers that can have multiple dimensions. A matrix is a “rank-2” tensor.↩︎\nTechnically, with 4 possibilities you only need 3 binary digits where [0, 0, 0] would be 100% probability of the fourth nucleotide↩︎\nI’m using a custom Dataset subclass. See source for details.↩︎\nTechnically a method since it’s in a class.↩︎\nAs an aside, the first time I wrote this I had all non-input nodes be Linear_blocks. This resulted in fair bit frusterated debugging as the network would either train perfectly or fail to train depending on how the last ReLU was initialized🤦🏼‍♂️.↩︎"
  },
  {
    "objectID": "posts/DanielKick/230915_solving_the_wrong_problem/index.html",
    "href": "posts/DanielKick/230915_solving_the_wrong_problem/index.html",
    "title": "Solving the Wrong Problem",
    "section": "",
    "text": "There’s a perspective that “it is better to know nothing than to know what ain’t so.”1 In my experience this is certainly the case with debugging because “knowing” will lead you down a rabbit trail of trying to solve the wrong problem.\nThe approach that works well for me is to “trust but verify” your knowledge. If your initial attempts to fix the bug in your code don’t work, take some time to check your assumptions – specifically your assumptions about where the bug is. This slows down your work initially because you’re often testing things that are behaving as you expect expectations, but this saves you from spending a lot of time trying to fix the wrong problems."
  },
  {
    "objectID": "posts/DanielKick/230915_solving_the_wrong_problem/index.html#what-you-know-that-aint-so",
    "href": "posts/DanielKick/230915_solving_the_wrong_problem/index.html#what-you-know-that-aint-so",
    "title": "Solving the Wrong Problem",
    "section": "",
    "text": "There’s a perspective that “it is better to know nothing than to know what ain’t so.”1 In my experience this is certainly the case with debugging because “knowing” will lead you down a rabbit trail of trying to solve the wrong problem.\nThe approach that works well for me is to “trust but verify” your knowledge. If your initial attempts to fix the bug in your code don’t work, take some time to check your assumptions – specifically your assumptions about where the bug is. This slows down your work initially because you’re often testing things that are behaving as you expect expectations, but this saves you from spending a lot of time trying to fix the wrong problems."
  },
  {
    "objectID": "posts/DanielKick/230915_solving_the_wrong_problem/index.html#a-recent-example",
    "href": "posts/DanielKick/230915_solving_the_wrong_problem/index.html#a-recent-example",
    "title": "Solving the Wrong Problem",
    "section": "A recent example:",
    "text": "A recent example:\nI’m writing a model that uses relationships between genes to predict a trait. The problem is that the model isn’t something I can write by hand (there are 6,067 inputs) and is way to slow – I’ve estimated it would take about 1.7 days to complete a single epoch (training cycle).\n\n\n\nHere are all the model’s processes for just two input genes.\n\n\nIn the diagram above, data from each gene (the nodes at the top) is fed into different functions (nodes 0-23) representing associations between different biological processes until they reach the output node (24) which predicts the trait.\nSome nodes share the same input (here node 14 and 10 both need node 11 as input). Under the hood I have the model storing the output of these nodes so it doesn’t have to re-calculate outputs un-necessarily (here the model would look-up the output of node 11 instead of recalculating it). This seems to work nicely but is a little unusual – in over two years this is the first time I’ve manually done this sort of thing.\nBecause of that, when I move my model from a tiny demo data set to the real thing and saw it was slow as molasses I “knew” my model slow because it was storing and retrieving intermediate results.\nOne assumption underlying this is that the model library is effectively designed and optimized such that it’s easier to get worse performance by doing unconventional things than better performance. This isn’t a bad assumption most of the time but we’ll see how it got me thinking about the wrong problem. My thought process went something like this:\n“Okay, so I’m doing something a little unconventional by looking up module outputs. Maybe if I can rewrite the model without this, some ☆Pytorch magic☆ will happen improving training speed.”\n“Hmm, the most straightforward way to write a model would be to chain the inputs and outputs like so”\ninput_1 = x_list[0]\nmodule_2 = x_list[1]\nintermediate_1 = module_1(input_1)\nintermediate_2 = module_2(input_2)\noutput  = module_3(nn.Concatenate([intermediate_1, intermediate_2], axis = 1))\n“But it would be unfeasible to do this because I’d have to write a line for each input and process 8,868 in total… or would it?”\nThis should have seemed like a totally unreasonable thing to do and been where I stopped to think if there was another way to get a speed increase (or tested this by writing a tiny neural net by hand with and without caching results and looked for a tiny difference in speed). However, years ago I met a class deadline by using python2 to write python2 code so this seemed perfectly feasible.\nSo the plan became :\n\nGenerate a boat load strings containing python code\nUse Python’s exec() and eval() functions to run each string\nSit back and think about what a clever idea it was having my code write my code.\n\nSeveral hours later I’ve learned a fair bit about how exec() and eval() handle scope and that their behavior between python2 and python3 has changed and still have no working model. So I decide to print the code I wanted executed to the console paste it (all 8,868 lines of it) into my model definition, and run it.\nThis solution was inelegant but quick to implement and exactly what needed to happen because the model didn’t perform any better. If anything it was slower, so there definitely wasn’t any ☆Pytorch magic☆ happening. This was a big enough surprise that it got me to question if the model was the problem after all instead of running down other rabbit trails."
  },
  {
    "objectID": "posts/DanielKick/230915_solving_the_wrong_problem/index.html#so-wheres-the-problem",
    "href": "posts/DanielKick/230915_solving_the_wrong_problem/index.html#so-wheres-the-problem",
    "title": "Solving the Wrong Problem",
    "section": "So where’s the problem?",
    "text": "So where’s the problem?\nBuilding a model may be the most evocative part of the data science workflow, but the steps that precede it are often as or more important. Choices around how to handle missing or potentially unrepresentative data are important as are how data is stored and moved around. In this case, I wasn’t thinking about these critical choices.\nFor each individual, there are data for genes throughout its genome (x_list, a list where each entry is a gene’s SNPs), and it’s trait of interest (y). Here’s the (simplified) code for this data set:\nclass ListDataset(Dataset):\n    def __init__(self, y, x_list):\n        self.y = y \n        self.x_list = x_list\n        \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        # Get the data for index `idx`\n        y_idx =self.y[idx]\n        x_idx =[x[idx, ] for x in self.x_list]\n        \n        # Move to GPU so model can access it\n        y_idx.to('cuda')\n        x_idx = [x.to('cuda') for x in x_idx]\n        return y_idx, x_idx\nDo you spot what’s happening? When __getitem__ loads an observation, it has to move data from each gene to the GPU. This process isn’t instantaneous and is happening for each of the 6,067 genes every time an observation is loaded.\nTraining a network with a mere 100 observations (batch size of 10) takes 101.89s/it but if all the data is moved to the GPU before its 15% faster at 86.34s/it.\nThat’s nice, but since there are over 80,000 observations, it’s not enough to make training this model feasible. There’s another place we can look for improvements, and that’s the batch size. Increasing the batch size will mean that more observations are being moved to the GPU at a time so it has to happen fewer times. In this example getting all training observations in a single batch makes training 86% faster at 13.44s/it."
  },
  {
    "objectID": "posts/DanielKick/230915_solving_the_wrong_problem/index.html#take-home",
    "href": "posts/DanielKick/230915_solving_the_wrong_problem/index.html#take-home",
    "title": "Solving the Wrong Problem",
    "section": "Take home:",
    "text": "Take home:\nTesting your assumptions (especially while debugging) is like insurance. When you’re on the right track from the start, it’ll cost you a little time that you otherwise wouldn’t have spent but it’ll keep you from spending a lot of time trying to solve the wrong problem.\npost script:\nEven solving the right problem the result may not be what you want. Extrapolating from a more realistic subset of the data results in an estimated 5.6 hours per epoch. Better than 1.7 days, but not a home run ."
  },
  {
    "objectID": "posts/DanielKick/230607_simulating_ensembles/index.html",
    "href": "posts/DanielKick/230607_simulating_ensembles/index.html",
    "title": "Simulation as a Super Power",
    "section": "",
    "text": "Writing simulations is one of the best ways I’m aware of to build one’s statistical intuition and comfort with data visualization. In addition to being able to try out new statistical tests and know exactly what effects they should find, they’re also great for communicating ideas and persuading others.\nA few months ago I had occasion to do just that.\n\\[...\\]\nAt the time I was advocating in a manuscript that when one needs to make a prediction combining predictions from different models is the way to go. Specifically, my results suggest that using a weighted average to make accurate models more influential. To do this, the predictions from each model are multiplied by the inverse of the model’s root mean squared error (rmse) of the model and summed. Someone helping me improve this manuscript thought that instead I should be weighting by the inverse of the model’s variance. This is a reasonable expectation (variance weighting is beautifully explained here) so I needed to convince my collaborator before the manuscript was ready for the world – Here’s how I did this with in a simulation that was only about 100 lines1 of R code."
  },
  {
    "objectID": "posts/DanielKick/230607_simulating_ensembles/index.html#simulating-observations",
    "href": "posts/DanielKick/230607_simulating_ensembles/index.html#simulating-observations",
    "title": "Simulation as a Super Power",
    "section": "Simulating Observations",
    "text": "Simulating Observations\nLet’s imagine we’re studying dead simple system where the output is equal to the input (\\(y = 0 + 1*x\\)). We can simulate as many samples as we would like from this system over a range of xs.\n\nn = 100 # samples to be generated\nxs &lt;- seq(0, 20, length.out = n) # x values evenly spaced from 0-20 \nys &lt;- 0+1*xs\n\nHere’s the simulated “data”.\n\n\n\n\n\n\n\n\nNow we can simulate models that are trying to predict y. To do this we’ll think of a model as being equivalent to the true value of y plus some model specific error. If we assume that the models aren’t prone to systematically over or underestimating, then we can use a normal distribution to generate these errors like so:\n\nmean1 =  0 # error doesn't tend to be over or under\nvar1  =  1 # variance of the error\ny1 &lt;- ys + rnorm(n = n, mean = mean1, sd = sqrt(var1))\n\nWe can simulate a better model by decreasing the variance (errors are consistently closer to the mean of 0). Conversely we can simulate a worse model by making the model tend to over or undershoot by changing the mean or make larger errors more common by increasing the varience. Here’s a model that’s worse than the previous one.\n\nmean2 =  1 # predictions tend to overshoot\nvar2  = 10 # larger errors are more common\ny2 &lt;- ys + rnorm(n = n, mean = mean2, sd = sqrt(var2))\n\nLet’s look at the predictions from model y1 and model y.\n\n\n\n\n\n\n\n\nHere we can see that y1’s error (vertical lines) are considerably smaller than that of y2.\nWe can subtract the true value \\(y\\) from the predicted value \\(\\hat y\\) to see this more clearly.\n\n\n\n\n\n\n\n\nIn panel B we can see the difference between the two error distributions for the models (save a few irregularities in these distributions from only using 100 samples.\nNow we can try out different averaging schemes to cancel out some of the error and get a better prediction. We can test a simple average like so.\n\ne1 &lt;- 0.5*y1 + 0.5*y2\n\nWe can also try placing more weight on models with less variable predictions (and hopefully smaller errors).\n\nyhat_vars  &lt;- unlist(map(list(y1, y2), function(e){var(e)})) # Calculate variance for each model's predictions\nwght_vars  &lt;- (1/yhat_vars)/sum(1/yhat_vars) # Take the inverse and get percent weight by dividing by the sum \ne2 &lt;- wght_vars[1]*y1  + wght_vars[2]*y2 # Scale each model's prediction and add to get the weighted average.\n\nWe can also try placing more weigh on models that are more accurate2.\n\nyhat_rmses &lt;- unlist(map(list(y1, y2), function(e){sqrt((sum((ys-e)**2)/n))}))\nwght_rmses &lt;- (1/yhat_rmses)/sum(1/yhat_rmses)\ne3 &lt;- wght_rmses[1]*y1 + wght_rmses[2]*y2\n\nNow we can calculate the RMSE for both models and these weighed averages.\n\n\n\n\n\nname\ny_rmse\nmean1\nmean2\nvar1\nvar2\n\n\n\ny1\n1.023765\n0\n1\n1\n10\n\n\ny2\n3.218318\n0\n1\n1\n10\n\n\nunif\n1.796390\n0\n1\n1\n10\n\n\nvar\n1.670037\n0\n1\n1\n10\n\n\nrmse\n1.217204\n0\n1\n1\n10\n\n\n\n\n\n\ny1 is the best set of predictions, averaging did not benefit predicitons here. This is not too much of a shock since y2 was generated by a model that was prone to systematically overshooting the true value and was more likely to have bigger errors.\nBut how would these results change if the models were more similar? What if the models had more similar error variences? Or if one was prone to overshooting while the other was prone to undershooting?"
  },
  {
    "objectID": "posts/DanielKick/230607_simulating_ensembles/index.html#expanding-the-simulation",
    "href": "posts/DanielKick/230607_simulating_ensembles/index.html#expanding-the-simulation",
    "title": "Simulation as a Super Power",
    "section": "Expanding the Simulation",
    "text": "Expanding the Simulation\nTo answer this we can package all the code above into a function with variables for each models’ error distribution and how many observations to simulate and return the RMSE for each model. This function (run_sim) is a little lengthy so I’ve collapsed it here:\n\nCoderun_sim &lt;- function(\n    n = 10000,\n    mean1 = 0,\n    mean2 = 1, \n    var1 = 1,\n    var2 = 10\n  ){\n  xs &lt;- seq(0, 20, length.out = n)\n  ys &lt;- 0+1*xs\n\n  # Simulate models\n  y1 &lt;- ys + rnorm(n = n, mean = mean1, sd = sqrt(var1))\n  y2 &lt;- ys + rnorm(n = n, mean = mean2, sd = sqrt(var2))\n  \n  # Equal weights\n  e1 &lt;- 0.5*y1 + 0.5*y2\n  \n  # Variance weights\n  yhat_vars  &lt;- unlist(map(list(y1, y2), function(e){var(e)}))\n  wght_vars  &lt;- (1/yhat_vars)/sum(1/yhat_vars)\n  e2 &lt;- wght_vars[1]*y1 + wght_vars[2]*y2\n  \n  # RMSE weights\n  yhat_rmses &lt;- unlist(map(list(y1, y2), function(e){sqrt((sum((ys-e)**2)/n))}))\n  wght_rmses &lt;- (1/yhat_rmses)/sum(1/yhat_rmses)\n  e3 &lt;- wght_rmses[1]*y1 + wght_rmses[2]*y2\n  \n  # Aggregate predictions and accuracy\n  data &lt;- data.frame(xs, ys, y1, y2, unif = e1, var = e2, rmse = e3)\n  plt_data &lt;- data %&gt;% \n    select(-xs) %&gt;% \n    pivot_longer(cols = c(y1, y2, unif, var, rmse)) %&gt;% \n    rename(y_pred = value) %&gt;% \n    # Calc RMSE\n    group_by(name) %&gt;%                \n    mutate(y_se = (ys - y_pred)**2) %&gt;% \n    summarise(y_rmse = sqrt(mean(y_se))) %&gt;% \n    ungroup() %&gt;% \n    mutate(mean1 = mean1,\n           mean2 = mean2,\n           var1 = var1,\n           var2 = var2)\n  \n  return(plt_data)\n}\n\n\nNext we’ll define the variables to examine in our computational experiment.\nWe can think about combining models that differ in accuracy (error mean) and precision (error variation). These differences can be are easier to think about visually. Here are the four “flavors” of model that we would like to combine to test all combinations of accuracy and precision.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\n\n\n\nSpecifically, We’ll have one model that acting as a stable reference and vary the error of the other (y2). We’ll make a version of the y2 model that is accurate (the mean error is only shifted by 0.01) and one that is inaccurate (the mean error is shifted by 50). Then we’ll see what happens when these two go from being precise (variance around 0.01) to very imprecise (variance up to 100).\nIn R this is expressed as below. When the mean of the error (mean_shift) is near zero accuracy is high. When the variance of the error (var_shift) is near zero precision is high.\n\nparams &lt;- expand.grid(\n  mean_shift = seq(0.01, 50, length.out = 2),\n  var_shift = c(seq(0.01, 0.99, length.out = 50), seq(1, 100, length.out = 90))\n)\n\nThis results in quite a few (280) combinations. of parameters Let’s look at the first and last few:\n\n\n\n\n\n\nmean_shift\nvar_shift\n\n\n\n1\n0.01\n0.01000\n\n\n2\n50.00\n0.01000\n\n\n3\n0.01\n0.03000\n\n\n278\n50.00\n98.88764\n\n\n279\n0.01\n100.00000\n\n\n280\n50.00\n100.00000\n\n\n\n\n\n\nNow we’ll generate the results. This code may look confusing at first. Here’s what it’s doing. 1. run_sim executes the steps we did above, using the parameters we specified in params to generate 100,000 observations and calculate the expected RMSE of each approach. 1. map is a way of looping over some input and putting all of the results into a list. In this case we’re looping over all the the rows in params so we will end up with a list containing a data.frame for each of the 280 parameter combinations. 1. rbind will combine two data.frames, ‘stacking’ one on top of the other. However, it can’t use a list as input so… 1. we have to use do.call. It will iteratively apply a function (rbind) to the entries in a list so that all the simulation results end up in one big data.frame.\n\nsim_data &lt;- do.call(         # 4.\n  rbind,                     # 3\n  map(seq(1, nrow(params)),  # 2.\n      function(i){           \n        run_sim(             # 1.\n          n = 10000,\n          mean1 = 0,\n          mean2 = unlist(params[i, 'mean_shift']), \n          var1 = 1,\n          var2 = unlist(params[i, 'var_shift']))\n      })\n)\n\nOnce this runs we can look at the results. Let’s consider the high accuracy y2 first, starting where y2’s variance is less than or equal to y1’s variance (1).\n\n\n\n\n\n\n\n\nWhen y2’s variance is very small (&lt; ~0.2) it outperforms all other estimates (just like the previous simulation). As it increases it crosses the line for \\(rmse^{-1}\\) weighting (rmse, orange line) and then the other averaging schemes before converging with y1 (dashed blue line). Over the same span \\(rmse^{-1}\\) converges with \\(var^{-1}\\) (var, red line), and the simple average (unif, black line).\n\n\n\n\n\n\n\n\nAs y2 continues to worsen, every prediction (except those from y1) get worse and worse. What’s interesting is that this doesn’t happen at the same rate. Because \\(rmse^{-1}\\) weighting penalizes predictions from models based on accuracy its error grows much more slowly than \\(var^{-1}\\) weighting or uniform weighting.\nTo summarize – If two models are equally good (1 on the y axis) then using any of the averaging strategies here will be better than not averaging. If one is far and away better than the other then it’s best to ignore the worse one. In practice one might find they have models that are performing in the same ballpark of accuracy. These results would suggest that in that case one gets the best results by \\(rmse^{-1}\\) weighting.\nNow let’s add in the case where one model is highly inaccurate. In this case, as precision worsens y2 (top blue line) has higher error but this is hard to see given just how much error it has to begin with. Uniform weighting follows a similar trend (but lessened by half) while \\(var^{-1}\\) improves as y2 becomes more imprecise because this decreases it’s influence on the final prediction. Of the averages \\(rmse^{-1}\\) is the best by a country mile because it accounts for the inaccuracy of y2 right from the start."
  },
  {
    "objectID": "posts/DanielKick/230607_simulating_ensembles/index.html#what-if-models-err-in-different-directions",
    "href": "posts/DanielKick/230607_simulating_ensembles/index.html#what-if-models-err-in-different-directions",
    "title": "Simulation as a Super Power",
    "section": "What if models err in different directions?",
    "text": "What if models err in different directions?\nJust for fun, let’s add one more simulation. Let’s suppose we have two models that are equally precise but err in opposite directions. We can modify the code above like so to have some combinations that are equally accurate (just in oppostie directions) and with differing accuracies.\n\nshift_array = seq(0.01, 10, length.out = 40)\nparams &lt;- expand.grid(\n  mean_shift  =    shift_array,\n  mean_shift2 = -1*shift_array\n)\n\nLet’s consider the case where errors are equal and opposite. In the previous simulation, when model variances were equal (1) the performance of all the averages converged, so we might expect that to be the case here. We can see the models getting worse and worse, but can’t see what’s happening with the averages.\n\n\n\n\n\n\n\n\nIf we zoom in, it looks like our intuition is correct (ignoring some sampling noise).\n\n\n\n\n\n\n\n\nBut we also simulated combinations where one model was off by more than the other. Let’s plot all the combinations of mean1 and mean2 but instead of showing the error of each method like we’ve done above, let’s instead just show where each method produces the best results.\n\n\n\n\n\n\n\n\nConsistent with what we’ve seen, for most of these combinations \\(rmse^{-1}\\) performs best. We can get a little fancier by color coding each fo these cells by the best expected error (y_rmse) and color coding the border with the method that produced the best expected error (excepting \\(rmse^{-1}\\) since that accounts for so much of the space).\n\n\n\n\n\n\n\n\nIt looks like there’s a sort of saddle shape off the diagonal. We’ll re-plot these data in 3d so we can usethe z axis for y_RMSE and color code each point as above.\n\n\n\n\n\n\nThere we go. Just a little bit of scripting and plotting will let one answer a whole lot of questions about statistics."
  },
  {
    "objectID": "posts/DanielKick/230607_simulating_ensembles/index.html#footnotes",
    "href": "posts/DanielKick/230607_simulating_ensembles/index.html#footnotes",
    "title": "Simulation as a Super Power",
    "section": "Footnotes",
    "text": "Footnotes\n\nI added a fair bit more for the sake of this post↩︎\nFor simplicity we’re not using testing and training sets. In this simulation that shouldn’t be an issue, but one might consider cases where this could matter. For instance if one model was wildly over fit then its RMSE would not be predictive of its RMSE on newly collected data↩︎"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200605_tips_ggplot_element_text/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200605_tips_ggplot_element_text/index.html",
    "title": "ggplot element_text() colors",
    "section": "",
    "text": "ggplot’s, element_text() comes with a color argument. Why does that matter? It doesn’t just accept atomics, you can hand it a vector! This effectively gives one access to conditional formatting. This works on ggplot2_3.3.0 but “vectorized input to element_text() is not officially supported” so YMMV with newer versions.\nHere’s an example I think it makes an otherwise unbearable figure a little more so without requiring duplicated labels.\nOne thing to be aware of is the ordering of a character/ factor may differ between a data.frame and the plot. In the visualized example, I had mRNA as type character instead of factor so it got alphabetized when it was plotted rather than by the order appearing in the data.frame."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/201104_r_resampling_demo/index.html",
    "href": "posts/DanielKick/210000_phd_tips/201104_r_resampling_demo/index.html",
    "title": "Resampling doesn’t need to be hard",
    "section": "",
    "text": "As an example to show how accessible resampling can be, here’s bit of code that resamples an anova and computes an empirical p value.\ntemp is a dataframe containing the data Condition is a column with exactly that temp_col is the name of a dependent variable. It’s a string to make this easy to reuse. if you haven’t used map before it’s basically a for loop that returns a list. When the output get’s passed into unlist it becomes an array.\ntemp_shuffle &lt;- temp\nresample_array &lt;- map(1:1000, function(i){\n     temp_shuffle$Condition &lt;- sample(temp_shuffle$Condition, replace = F)\n     fm &lt;- lm(as.formula(paste0(temp_col, \" ~ Condition\")), data = temp_shuffle)      \n     return(car::Anova(fm)[1,3])\n}) %&gt;% unlist()\nep &lt;- mean(resample_array &gt;= car::Anova(fm)[1,3])\nThe down side is that it takes orders of magnitude more time to run because you’re running the same code hundreds or thousands of times. This is only a problem if you need crazy high precision or have a really complex/hard to fit model. For reference using the code above took about ~2 seconds/dv for 1000 iterations on my machine.\nA handy pattern is to use map to summarize data and then bind it.\nmap_res &lt;- map(names(M)[names(M) != \"Sample\"], function(i){\n  res &lt;- shapiro.test(M[[i]])\n\n  return(\n    list(\n    mrna = i,\n    p = res$p.value\n    )\n  )\n})\n\nshapiro_res &lt;- do.call(rbind, map_res)"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200506_tips_ggplot_factors/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200506_tips_ggplot_factors/index.html",
    "title": "ggplot: Beware of Factors!",
    "section": "",
    "text": "Don’t get suckered when converting factors! Numeric data can be assigned to a factor type which will throw a wrench in a plot or analysis (1/3) \nA knee jerk reaction would be to convert it to a numeric with as.numeric(). That doesn’t work either. (2/3) \nHowever if you use as.numeric(as.character()) then it works. That’s because factors are ordinal and named so if you convert the type to character first to ensure R is working with the factor names instead of the ranks. (3/3)"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200519_tips_ggplot_shading_with_geom_rect/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200519_tips_ggplot_shading_with_geom_rect/index.html",
    "title": "Shading with geom_rect",
    "section": "",
    "text": "Use geom_rect() with the min/max set to -Inf and Inf to add a pleasant shading to your facets.\nFor example by passing it a data frame with the faceting variables and a column to use for the color (green if positive, red if negative) we can make facets behave like cells in a heatmap!\n# &gt; tp2\n# # A tibble: 23 x 5\n#    Condition   Trace             Time  Change StimId\n#    &lt;fct&gt;       &lt;chr&gt;             &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt; \n#  1 PS.0.orig   170623b_0007.abf  60         1 a     \n#  2 PS.45.orig  170825a_0008.abf  60        -1 c     \n\nggplot(df)+\n  geom_rect(data = tp2, aes(fill = Change),xmin = -Inf,xmax = Inf, ymin = -Inf,ymax = Inf,alpha = 0.3) +\n  geom_hline(yintercept = 0, color = \"cornflowerblue\")+\n  geom_pointline(aes(x = Time, y = rc, group = Experiment), shape = 1, color = \"black\")+\n  ylim(-2, 3.5)+\n  scale_fill_gradientn(colors = c(\"Red\", \"Grey\", \"Green\"))+\n  theme_base()+\n  theme(legend.position = \"\")\n\n\n\nimage (5).png"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200504_tips_r_faster_install/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200504_tips_r_faster_install/index.html",
    "title": "Upgrading R versions on Windows",
    "section": "",
    "text": "If you’re on windows, installr should allow you to copy over the libraries from previous versions."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/210202_ggplot_ggcorrplot/index.html",
    "href": "posts/DanielKick/210000_phd_tips/210202_ggplot_ggcorrplot/index.html",
    "title": "ggplot frienndly corrlation plots with ggcorrplot",
    "section": "",
    "text": "A co-worker ran into issues with corrplot::corrplot() cutting off the title. A useful alternative is ggcorrplot. It makes okay plots with ggplot2’s logic. Not as clean as the above but it’ll work with patchwork and cowplot. Unfortunately, scale_colour_stepsn doesn’t override the scaling.\nlibrary(ggcorrplot)\n\np.mat &lt;- cor_pmat(cor_df)\nggcorrplot(cor(cor_df, use = \"pairwise.complete.obs\"), \n           p.mat = p.mat,\n           insig = \"blank\",\n           type = \"upper\",\n           outline.col = \"white\",\n           colors = RColorBrewer::brewer.pal(n = 9, name = \"PuOr\")[c(1,5,9)]\n           )+\n  labs(title = \"Brian_AP_Delayed\")\n\n\n\nimage (19).png\n\n\nggcorrplot appears to call internal functions which makes modifying it quickly impractical (one would probably be best forking the package and modifying that). I think I have a workaround that gets the same binning behavior:\nAfter the significance matrix (p.mat) is generated overwrite the correlation matrix with the middle value of each desired bin.\n          bkkca      cav1      cav2\nbkkca 1.0000000 0.3452702 0.5603564\ncav1  0.3452702 1.0000000 0.7880727\ncav2  0.5603564 0.7880727 1.0000000\n&gt; # bin the correlations so there are fewer colors used in the figure\n&gt; cor_bins &lt;- seq(-1, 1, length.out = 9)\n&gt; for (i in 1:(length(cor_bins)-1)){\n+   test[test &gt; cor_bins[i] &amp; test &lt; cor_bins[i+1]] &lt;- ((cor_bins[i] + cor_bins[i+1])/2)\n+ }\n&gt; test\n      bkkca  cav1  cav2\nbkkca 1.000 0.375 0.625\ncav1  0.375 1.000 0.875\ncav2  0.625 0.875 1.000\nHere this makes very slight changes to the plot. (legend dropped to not imply a continuous fill) \n(2021-2-2) Last update, this is harder to read up will use the more extreme value to get closer to corrplot\ntest &lt;- seq(-1, 1, length.out = 5)+.0000001\ntest\n#-0.9999999 -0.4999999  0.0000001  0.5000001  1.0000001\nfor (i in 1:(length(cor_bins)-1)){\n  # test[test &gt; cor_bins[i] &amp; test &lt; cor_bins[i+1]] &lt;- (cor_bins[i] + cor_bins[i+1])\n  test[test &gt; cor_bins[i] &amp; test &lt; cor_bins[i+1]] &lt;-   cor_bins[c(i, (i+1))[which.max(abs(cor_bins[i:(i+1)]))]]\n}\ntest\n#-1.00 -0.50  0.25  0.75  1.00\n\n\n\nimage (21).png"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200410_tips_furrr_is_multithreaded_purrr/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200410_tips_furrr_is_multithreaded_purrr/index.html",
    "title": "Parallel Processing for purrr with furrr",
    "section": "",
    "text": "If you’re iteratively making plots, resampling, or doing another task that would have your reach for a for loop or lapply() use furrr::future_map() instead. furrr gives parallel processing ready versions of tidyverse’s purrr functions (e.g. map(), walk()). It’s easy to install the dependencies and takes a lot of the headache out of parallel processing."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200420_tips_r_rm/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200420_tips_r_rm/index.html",
    "title": "Clearing all but certain objects",
    "section": "",
    "text": "If you run into a situation where you’re using a ton of memory (e.g. manipulating transcriptomic data, resampling, or working with electrophysiology traces) use rm() to selectively get rid off objects in the environment. A useful pattern is to write out large objects you’ll need in the future, remove them, and then read them back in when you need them. This is usually not important, but when it is, it is.\nIf you’re working interactively and tempted to use rm(list = ls()) consider restarting your r session (ctrl+shift+F10 on windows). Overreliance on rm(list = ls()) is poor form.\nA side note – unlike listing items where the function matching the unix command acts on the environment and a new command acts on the files system (ls() and list.files()) the functions for removing items don’t follow this logic. rm() acts on objects in your environment whereas unlink() acts on system files.\nSimilarly if you want to retain only specific objects you can take this approach:\n # get rid of everything\nrm(list=ls())\n\n# get rid of everything except specific objects and all loaded functions\nrm(list = \n   ls()[!(ls() %in% c(\n   # objects\n   c(\"data1\", \"data2\", \"bool1\", \"list1\"), \n   # functions\n   lsf.str())\n   ) ])"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/210907_tips_r_approximate_string_matching/index.html",
    "href": "posts/DanielKick/210000_phd_tips/210907_tips_r_approximate_string_matching/index.html",
    "title": "Approximate String Matching",
    "section": "",
    "text": "fuzzywuzzy is a tool that isn’t necessary most of the time but when it is it can save a ton of time. It lets you do approximate string matching. I’ve used it for handling typos and differences in white space/punctuation/naming conventions in entry labels and it’s worked nicely. There’s a port for R and a few other languages too."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/201209_diagrams_as_code/index.html",
    "href": "posts/DanielKick/210000_phd_tips/201209_diagrams_as_code/index.html",
    "title": "Diagrams as code",
    "section": "",
    "text": "Check out DiagrammeR (or mermaid) if you need clean easy flow diagrams. I find they’re not to hard to make and even easier to forget that you’ve made them.\nDiagrammeR::grViz(\"digraph {\ngraph [layout = dot]\n\n# define the global styles of the nodes. We can override these in box if we wish\nnode [shape = circle, style = filled, fillcolor = LightSteelBlue]\n\ndata1 [label = 'Input', shape = folder, fillcolor = Beige]\n\nglmnet [label = 'Lasso \nRegression', shape = box, fillcolor = Linen]\nmnnet [label = 'Mulitnomial \nNeural \nNetwork', shape = box, fillcolor = Linen]\nnnet [label = 'Neural \nNetwork', shape = box, fillcolor = Linen]\nknn [label = 'k-Nearest \nNeighbor', shape = box, fillcolor = Linen]\nranger [label = 'Random \nForest', shape = box, fillcolor = Linen]\nsvml [label = 'SVM \nLinear', shape = box, fillcolor = Linen]\nsvmr [label = 'SVM \nRadial', shape = box, fillcolor = Linen]\n\ndata2 [label = '5-fold CV \nAccuracy', shape = folder, fillcolor = Beige]\n\n# edge definitions with the node IDs\ndata1 -&gt; {glmnet mnnet nnet knn ranger svml svmr} -&gt; data2\n{alpha lambda} -&gt; glmnet\ndecay -&gt; mnnet\n{size decay} -&gt; nnet\nk -&gt; knn\n{mtry splitrule minNodeSize} -&gt; ranger\ncost -&gt; svml\nsigma -&gt; svmr\n}\")\n\n\n\nimage (15).png"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/201222_ggplot_font_custom/index.html",
    "href": "posts/DanielKick/210000_phd_tips/201222_ggplot_font_custom/index.html",
    "title": "ggplot font customization",
    "section": "",
    "text": "You can set ggplot’s font using the theme function. Particularly if combined with functions from ggthemes or ggsci you can get very pleasing visualizations quickly. Beyond accessing fonts already on your system you can import and fonts with minimal hassle.\ne.g. to get the font Metal Mania ready to use one might run:\nlibrary(showtext)\nfont_add_google(name = \"Metal Mania\", family = \"Metal+Mania\")\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(patchwork) # for adding together plots at the end\nlibrary(extrafont)\n# font_import()) # &lt;--- run this once\nloadfonts(device = \"win\", quiet = TRUE) # &lt;--- run this once per session\n# extrafont::fonts() # see fonts that are available\n\nplt1 &lt;- palmerpenguins::penguins %&gt;%\n  filter(!&lt;http://is.na|is.na&gt;(sex)) %&gt;%\n  mutate(sex = case_when(sex == \"male\" ~ \"m\",\n                         sex == \"female\" ~ \"f\")) %&gt;% \n  ggplot(aes(sex, body_mass_g, fill = species, group = interaction(species, sex)))+\n  geom_boxplot()+\n  ggthemes::scale_fill_colorblind()+\n  ggthemes::theme_clean()+\n  theme(legend.position = \"\")+\n  facet_grid(.~species)+\n  labs(title = \"Default\")\n\nplt2 &lt;- plt1+\n  theme(text = element_text(family = \"Consolas\"))+\n  labs(title = \"Consolas\")\n\nplt3 &lt;- plt1+\n  theme(text = element_text(family = \"Garamond\"))+\n  labs(title = \"Garamond\")\n\nplt1 + plt2 + plt3\n\n\n\nimage (17).png"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200730_use_roxygen2/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200730_use_roxygen2/index.html",
    "title": "Document your functions with `roxygen2``",
    "section": "",
    "text": "If you want to make future you more positively disposed to present you, then organize your work with R packages, save your custom functions in the /R/ directory. The function roxygen2::roxygenise() will documentation comments for your functions into help pages. For example running roxygenise() with the below function saved produces the attached help page accessible via ?shrug.\n#' @title Print Shrug\n#' @description This function prints a shrug emoji a specified number of times, provided the input value is a numeric greater than zero.\n#' @param n how many shrugs should be printed\n#' @author Daniel Kick (\\email{daniel.r.kick@@gmail.com})\n#' @export\n#' @examples\n#' shrug(5)\n\nshrug &lt;- function(n = 1, ...){\n  if (is.numeric(n) & n&gt;0){\n    for (i in seq(1, n)){\n        cat(\"¯\\_(ツ)_/¯\n\")\n    }\n  }\n}\n\n\n\nimage (12).png"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/201120_tips_r_roman_numerals/index.html",
    "href": "posts/DanielKick/210000_phd_tips/201120_tips_r_roman_numerals/index.html",
    "title": "Roman numeral convenince function",
    "section": "",
    "text": "For all you classics folks out there, R has a convenience function just for you! (As long as your numbers aren’t too big). Credit goes to Georgios Karamanis @geokaramanis for teaching me this.\n&gt; as.numeric(as.roman(\"MCXXIII\"))\n[1] 1123\n&gt; as.roman(1123)\n[1] MCXXIII\n&gt; as.roman(11234)\n[1] &lt;NA&gt;"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/210203_thought_exp_replicated_res/index.html",
    "href": "posts/DanielKick/210000_phd_tips/210203_thought_exp_replicated_res/index.html",
    "title": "Thought Experiment: Comparing Replicate Experiments’ Conclusions",
    "section": "",
    "text": "A collaborator and I just talked through how one might compare two sets of p values. We’re thinking about the following approach.\nSetup: You have two replicates of an experiment (r1, r2). In each experiment you measured three mRNAs (a, b, c) in control and treatment (c, t). You want to know if the same trends in r1 show up in r2 but there is a batch effect that will prevent comparing them directly (e.g. you can’t run a t test on r1 _a_ c and r2 _a_ c )\nProposed Solution:\n\nCompare each mRNA within a replicate and note the sign of change and if the p value reached a pre-determined cutoff (we lose some information doing this since we’re pooling 0.06 and 0.99 together)\n\nrep  |mrna |sign |pval |sig  |\nr1   | a   | -   |0.04 |1    |\nr1   | b   | +   |0.10 |0    |\n...  |     |     |     |     |\nr2   | c   | -   |0.02 |1    |\n\nMultiply the sign by the significance code so that -1 = “significant decrease”, 0 = “no significant change”, +1 = “significant increase”\n\nrep  |mrna |sign |pval |sig  |sxp  |\nr1   | a   | -   |0.04 |1    |+1   |\nr1   | b   | +   |0.10 |0    |0    |\n...  |     |     |     |     |     |\nr2   | c   | -   |0.02 |0    |-1   |\n\nReshape these as two vectors and treat them as categorical data. Then compare the “assignment” between these two lists using a jaccard index as if we were comparing an assignment from clustering against reality.\n\nr1 &lt;- as.character(c(1, 0, 0))\nr2 &lt;- as.character(c(0, 0, -1))\njaccard(r1, r2)\n\nUse resampling to find an empirical p value for this observed jaccard index.\n\nDoes that seem reasonable? Is there another way you would go about it? (Email me what you think!)\nHere’s an implementation for two sets of correlations. Here we bin the correlations into 5 bins use a jaccard index to assess whether the bin assignments are the same for both datasets (Brian’s and mine). To confirm that the measured jaccard index (0.23) isn’t anything to write home about we can generate an empirical p value (ep = 0.13).\n# cor_comp_df\n#\n#    Source Time     x     y        Corr\n#    &lt;fct&gt;  &lt;fct&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n#  1 Person1  Baseline bkkca cav1    0.441\n#  2 Person1  Baseline bkkca cav2    0.476\n#  3 Person1  Baseline bkkca inx1    0.435\n#  4 Person1  Baseline bkkca inx2    0.159\n#  5 Person1  Baseline bkkca inx3   -0.174\n#  6 Person1  Baseline bkkca inx4   -0.167\n\nn_bins &lt;- 5\n\ntemp &lt;- cor_comp_df %&gt;% \n  mutate(Bins = cut(Corr, breaks = seq(-1, 1, length.out = n_bins))) %&gt;% \n  select(-Corr) %&gt;% \n  pivot_wider(names_from = \"Source\", values_from = \"Bins\")\n\n# temp\n#\n#   Time     x     y     Person1    Person2  \n#   &lt;fct&gt;    &lt;chr&gt; &lt;chr&gt; &lt;fct&gt;    &lt;fct&gt;   \n# 1 Baseline bkkca cav1  (0,0.5]  (0,0.5] \n# 2 Baseline bkkca cav2  (0,0.5]  (0,0.5] \n# 3 Baseline bkkca inx1  (0,0.5]  (0.5,1] \n# 4 Baseline bkkca inx2  (0,0.5]  (0.5,1] \n# 5 Baseline bkkca inx3  (-0.5,0] (0.5,1] \n# 6 Baseline bkkca inx4  (-0.5,0] (-0.5,0]\n\n\nlibrary('clusteval')\nobs_jaccard &lt;- cluster_similarity(temp$Person1, temp$Person2, similarity=\"jaccard\")  \n\n# 0.2304234\n\nnull_jaccard &lt;- map(1:10000, function(i){\n  cluster_similarity(sample(temp$Person1, replace = F), \n                     temp$Person2, similarity=\"jaccard\")\n  }) %&gt;% \n  unlist()\n\n\ntemp &lt;- with(density(null_jaccard), data.frame(x, y))\ntemp &lt;- temp %&gt;% mutate(xmax = max(x),\n                obs = obs_jaccard)\n\nggplot(data = temp, aes(x = x, y = y))+\n  geom_line()+\n  geom_vline(xintercept = obs_jaccard)+\n  geom_ribbon(data = temp[temp$x &gt; temp$obs, ], \n              aes(xmin = obs, xmax = xmax, ymin = 0, ymax = y))+\n  labs(subtitle = paste(\"empirical p=\", as.character(mean(null_jaccard &gt;= obs_jaccard))))\n\n\n\nimage (22).png\n\n\nIt’s worth generating an empirical p value for each comparison you’re making. For example here I’m comparing the results of an experiment replicate. Each dependent variable is assigned a group based on if one would conclude it there was a difference (0 or 1) between groups and the sign of that difference (+ or -). Seeing a Jaccard index of 0.61 (out of 1) we might conclude we replicated most of the findings. However, the empirical p value is 1 because most of the comparisons were non-significant in both groups resulting in a high floor for the index."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200519_tips_ggplot_greek_letters/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200519_tips_ggplot_greek_letters/index.html",
    "title": "Greek Letters in ggplot",
    "section": "",
    "text": "If you need to include greek letters or special characters in a plot use annotate() instead of geom_text(). On my machine it preforms a lot faster.\n# 57.62977 secs\ngeom_text(aes(x = 15, y = 25, label = \"phi~22.5\"), parse=TRUE)\n# 1.988642 secs\nannotate(\"text\", x = 15, y = 25, parse = TRUE, label = as.character(expression(paste(phi, \" 22.5\"))))"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/210305_tips_ggplot_markdown/index.html",
    "href": "posts/DanielKick/210000_phd_tips/210305_tips_ggplot_markdown/index.html",
    "title": "ggplot tips: markdown for italics in plots and adding breaks to color scaling",
    "section": "",
    "text": "Two tricks today: 1. Scale fill functions can accept breaks and limit arguments so you don’t have to use hacky workarounds like binning the data before plotting (which is what I usually do). 2. library(ggtext) lets you render markdown within plots (e.g. for those pesky mRNAs)\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nset.seed(42)\ndf &lt;- do.call(cbind, map(\n  1:5, \n  function(e){\n    data.frame(gene = rnorm(10))\n    })\n  )\n\ndf &lt;- df |&gt;\n  corrr::correlate() |&gt;\n  corrr::shave() |&gt; \n  pivot_longer(cols = starts_with('gene')) |&gt; \n  rename(term2 = name, Cor = value) |&gt; \n  drop_na()\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\nhead(df)\n\n# A tibble: 6 × 3\n  term   term2      Cor\n  &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n1 gene.1 gene   -0.375 \n2 gene.2 gene    0.440 \n3 gene.2 gene.1 -0.153 \n4 gene.3 gene   -0.404 \n5 gene.3 gene.1  0.465 \n6 gene.3 gene.2 -0.0600\n\n\nBefore:\n\ndf |&gt; \n  ggplot(aes(term, term2, fill = Cor))+ \n  geom_tile()+ labs(x = \"mRNA\", y = \"\")+ \n  scale_fill_distiller(palette = \"PuOr\")+ \n  coord_fixed()\n\n\n\n\n\n\n\nAfter:\n\nlibrary(ggtext) # https://github.com/wilkelab/ggtext\nlibrary(glue)\n\ndf |&gt; \n  mutate(term = glue((\"&lt;i&gt;{term}&lt;/i&gt;\"))) |&gt;\n  ggplot(aes(term, term2, fill = Cor))+ \n  geom_tile()+ labs(x = \"mRNA\", y = \"\")+ \n  theme(axis.text.x = element_markdown(angle = 45))+ # &lt;-- Note that we have element_markdown not element_text\nscale_fill_stepsn(\n  colors=RColorBrewer::brewer.pal(n = 8, name = \"PuOr\"), \n  na.value = \"transparent\", breaks=round(seq(-1, 1, length.out =8), digits = 2), \n  limits=c(-1,1) )"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/220531_tips_ggplot_invisible_hexcode/index.html",
    "href": "posts/DanielKick/210000_phd_tips/220531_tips_ggplot_invisible_hexcode/index.html",
    "title": "ggplot and the invisible hex code.",
    "section": "",
    "text": "If you use an eight digit hex code for specifying a color value, the first two control transparency. Thus, you can set fill = “#00000000” in ggplot to get a boxplot with no fill. Here’s a (somewhat contrived) use case: points on top occlude the cross bar but box on top hides the observations.\n\nlibrary(ggplot2)\ndata.frame(y = 0, x = 1:9) |&gt;\n  ggplot(aes(x = x, y = y))+\n  geom_point(color = 'firebrick')+\n  geom_boxplot(fill = '#00000000')"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200512_tips_r_cache_results/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200512_tips_r_cache_results/index.html",
    "title": "Cache Intermediate Results",
    "section": "",
    "text": "Save R objects as rds. These could be works in progress or items that take a while to generate (e.g. re-sampled results). This gives the utility of saving a work space without the dangers.\nlibrary(here)\nsave(df, file = here(\"data\", \"df.rds\"))\n# df is now saved at ./data/df.rds\n\nload(here(\"data\", \"df.rds\"))\n# df is now loaded from ./data/df.rds as df"
  },
  {
    "objectID": "posts/DanielKick/220323_python_caching_pickle/index.html",
    "href": "posts/DanielKick/220323_python_caching_pickle/index.html",
    "title": "Tips: Cache Intermediate Results with pickle",
    "section": "",
    "text": "Here’s a useful pattern I’ve been getting a lot of mileage out of lately. If you’re running an analysis that has a time consuming step you can save the result as a python readable “pickle” file. Addendum: In some cases pickling a python objects can sometimes succeed in storing and retrieving data where a library’s built in functions for saving/loading data fails.\nimport pickle as pkl\n\npath = \"./data_intermediates/processed_data.pkl\"\nif os.path.exists(path):\n    processed_data = pkl.load(open(path, 'rb'))\nelse:\n    # Make `processed_data` here\n    pkl.dump(processed_data, open(path, 'wb'))\nThis also lets you batch a process so that you can do more with your resources. For example here’s a list comprehension that will (for each day from 0-287) rearrange the weather data to be in “long” format. This is concise but requires processing the whole list at once which takes a lot of resources.\nsal_long_list = [_get_weather_long(results_list = res,\n                                   current_day = ith_day) for ith_day in np.linspace(start = 0, stop = 287, num = 288)]\nIf we incorporate it into the pattern above we can hold fewer items in memory at a time and then merge them (e.g. with list.extend() ) after the fact.\nfor ii in range(3):\n    file_path = '../data/result_intermediates/sal_df_W_long_part_day'+['0-95', \n                                                                       '96-191', \n                                                                       '192-287'][ii]+'.pkl'\n    if os.path.exists(file_path):\n        sal_long_list = pkl.load(open(file_path, 'rb'))\n\n    else:\n        # The original list comprehension is here, \n        # just made messier by selecting a subset of the indices.\n        sal_long_list = [_get_weather_long(                                \n            results_list = res,\n            current_day = current_day) for current_day in [\n            [int(e) for e in np.linspace(start = 0, stop = 95, num = 96)],   # Batch 1\n            [int(e) for e in np.linspace(start = 96, stop = 191, num = 96)], # Batch 2\n            [int(e) for e in np.linspace(start = 192, stop = 287, num = 96)] # Batch 3\n        ][ii]\n        ]\n        pkl.dump(sal_long_list, open(file_path, 'wb'))"
  },
  {
    "objectID": "posts/DanielKick/240516_r_qr_email/index.html",
    "href": "posts/DanielKick/240516_r_qr_email/index.html",
    "title": "Tip: Use R to create an email nudge",
    "section": "",
    "text": "Suppose you’re giving a presentation and you want to make it easy for people to contact you afterwards. Maybe you have your email in the acknowledgements or maybe you make business cards with a qr code to your website.\nThese are good steps but we can go further. Let’s make qr code that nudges people to send an email. I’ve used this to good effect for getting emails of people who would like to be notified when a software project goes live.\nHere’s the plan: 1. Give people your email. 2. Make it easy for them to send you an email. 3. Encourage them to do it now.\nTo accomplish this we’re going to create a mailto link and encode it as a qr code. mailtos are opened in your default mail application so this gets the address where it’ll be used with zero typing. We’ll add some suggested text to the email. This gives the user a starting point and gives us a default subject line to search for later.\nHere’s what this looks like in R. After setting the email and default subject and body text the spaces are replaced with %20 (20 is the ASCII hexdecimal code for space). We concatenate these strings together and then use the marvelous qrcode library to make a graphic that’s ready for a poster, presentation, or card.\n\nlibrary(qrcode)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nemail    = 'your.name@usda.gov'\ninp_subj = 'Re: Envriomental Deep Learning Presentation'\ninp_body = 'Hello from the conference...'\n\n\ninp_subj = str_replace_all(inp_subj, ' ', '%20')\ninp_body = str_replace_all(inp_body, ' ', '%20')\n\ninp_text = paste0(c(\n  '&lt;a href=\"mailto:',email,'?&subject=', inp_subj, '&body=', inp_body, '&gt; &lt;/a&gt;'\n), collapse = '')\n\nplot(qrcode::qr_code(inp_text))"
  },
  {
    "objectID": "posts/DanielKick/230912_vignettes_for_tacit_knowledge/index.html",
    "href": "posts/DanielKick/230912_vignettes_for_tacit_knowledge/index.html",
    "title": "Capturing Tacit Knowledge Through Blogging",
    "section": "",
    "text": "One of my goals with mentoring is to help others avoid places I got stuck (or get through them more easily) when I was learning. Part of this is writing guides, documentation, SOPs, etc. that I wish I had, but there’s some knowledge that doesn’t fit into a nice long form documents.\nTacit knowledge (e.g. how to approach different problems) or helpful little tricks aren’t well suited to that sort of format. Posts in this series are designed to collect thoughts in this vein to hopefully help out a fellow computational traveler starting their journey.\nTo that end these posts will be a mix of tricks, advice, retrospectives, and maybe a few guides that don’t fit in the protocols/ folder."
  },
  {
    "objectID": "posts/DanielKick/230912_vignettes_for_tacit_knowledge/index.html#advice-à-la-carte",
    "href": "posts/DanielKick/230912_vignettes_for_tacit_knowledge/index.html#advice-à-la-carte",
    "title": "Capturing Tacit Knowledge Through Blogging",
    "section": "",
    "text": "One of my goals with mentoring is to help others avoid places I got stuck (or get through them more easily) when I was learning. Part of this is writing guides, documentation, SOPs, etc. that I wish I had, but there’s some knowledge that doesn’t fit into a nice long form documents.\nTacit knowledge (e.g. how to approach different problems) or helpful little tricks aren’t well suited to that sort of format. Posts in this series are designed to collect thoughts in this vein to hopefully help out a fellow computational traveler starting their journey.\nTo that end these posts will be a mix of tricks, advice, retrospectives, and maybe a few guides that don’t fit in the protocols/ folder."
  },
  {
    "objectID": "posts/DanielKick/231229_scheduling_worse_is_better/index.html",
    "href": "posts/DanielKick/231229_scheduling_worse_is_better/index.html",
    "title": "Worse is better case study 1",
    "section": "",
    "text": "Slurm1 is a tool commonly available on computing clusters for scheduling job. For a while I’ve wanted a local instalition of it so I can queue computational experiments to run while I’m sleeping or gone. Trouble is, it’s never been a high enough priority to devote much time to getting it set up (or convince a sysadmin to help me set it up). Every few months I’ll work through a tutorial until I find I’ve exhausted the time set aside for the task without a working installation. After the most recent cycle of this I resolved to build an imperfect solution instead."
  },
  {
    "objectID": "posts/DanielKick/231229_scheduling_worse_is_better/index.html#thinking-throught-the-system-requirements",
    "href": "posts/DanielKick/231229_scheduling_worse_is_better/index.html#thinking-throught-the-system-requirements",
    "title": "Worse is better case study 1",
    "section": "Thinking throught the system requirements",
    "text": "Thinking throught the system requirements\nLet’s start with what I want my solution to do:\n\nRun jupyter notebooks\nMaintain a queue of jobs to run\nMake sure that resources (here the GPU’s memory) are free for the next job and run jobs as other jobs finish2\nControl the system (add jobs to be run and such)\n\nJupyter notebooks can be run from the command line in a virtual environment (e.g.conda run -n gpu_env jupyter execute notebook0.ipynb).\nA queue could be as simple as a list of notebooks to be run (maintained in a list or a text file) so this requirement is easy to satisfy. Without the In the simplest conception, the jobs could listed (separated by a ;) and run from the command line and each would run in turn. This would not allow for modifying the queue however.\nFreeing up resources is and starting new jobs is more challenging. Ideally when the notebook finishes running it should release the resources but what if there’s an error or bug the process release them? In a notebook we could include os._exit(00) in the last cell to kill the process but if the notebook runs correctly this shouldn’t be an issue. As a fail safe I could keep an eye on what new programs are using the GPU and if they don’t release memory stop them with kill -9 $PID. Not a pretty solution (and if I start another process that uses the GPU, it could get killed) but it will work.\nLastly is a way to control the system. There needs to be a way to modify it’s state even if it’s running in the background. A simple way to do this would be to specify commands files in a specific location and naming convention."
  },
  {
    "objectID": "posts/DanielKick/231229_scheduling_worse_is_better/index.html#implementation",
    "href": "posts/DanielKick/231229_scheduling_worse_is_better/index.html#implementation",
    "title": "Worse is better case study 1",
    "section": "Implementation",
    "text": "Implementation\nI’m using python for this tool since I find it’s more readable than bash. The notebooks I want to schedule are in a single directory so I’ll add a file, SchedulerControlCenter.py, there and a folder for the control files I’ll use to modify the scheduler’s state.\n.\n├── notebook0.ipynb\n├── notebook1.ipynb\n├── ...\n├── SchedulerControlCenter.py\n└── SchedulerFiles\n    └── ctrl0.json\n\nControl\nStarting with the control files, I’d like to be able to add, remove, and move jobs in the queue, prevent a process from being closed (if I’m using the GPU while this is running), print information on the system’s state, and provide some notes to the user (in case I forget details later).\nHere’s a dictionary with keys corresponding to each of these cases.\ndata = {\n    'info'                :[],                  # Print this message\n    'nvidia_base_pids_add':['40082'],           # Prevent a specific PID from being autoclosed. \n    'nvidia_base_pids_del':['40082'],           # Allow a specific PID to be autoclosed.\n    'ipynb_names_read'    :[],                  # Print currently queued notebooks.\n    'ipynb_names_add'     :['notebook0.ipynb'], # Add a notebook (to the end) of the queue\n    'ipynb_names_next'    :['notebook0.ipynb'], # Add a notebook to the beginning of the queue (does not need to be in the queue)\n    'ipynb_names_del'     :['notebook0.ipynb'], # Remove a notebook from the queue\n}\nTo make things easy I’ll create a function to write a dictionary as json and autoname the file (so I don’t overwrite unprocessed commands).\nimport json\n\ndef write_ctrl_json(data: dict):\n    ctrl_files = [e for e in os.listdir('./SchedulerFiles/') if (re.match('ctrl.json', e) or re.match('ctrl\\d+.json', e))]\n    # find the next number to write to. In case 'ctrl.json' and 'ctrl0.json' exist, I'll write to 1\n    max_num = [e.replace('ctrl', '').replace('.json', '') for e in  ctrl_files]\n    max_num = [int(e) for e in max_num if e != '']\n    if max_num == []: max_num = 0\n    else: max_num = max(max_num)\n\n    with open(f'./SchedulerFiles/ctrl{max_num+1}.json', 'w', encoding='utf-8') as f:\n        json.dump(data, f, ensure_ascii=False, indent=4)\n\n\nThe Scheduler\n\nSetup\nIn practice I’ll begin by writing these control files then start the scheduler. I’ll need to parse the instructions, run the first job, check for new instructions, then run the next job and then repeat this until all jobs are complete. In between jobs, I’ll compare the processes running on the GPU and if any are not supposed to be there, stop them.\nI’ll keep track of the original state (nvidia_base_pids) of the GPU using a list of the PIDs that were running on it and current state as a dictionary (nvidia_state) which makes it easy to keep additional information for future use (e.g. how much memory is being used). The queue itself (ipynb_names) can be a simple list of the files to be run. These will need to be populated either from the GPU’s current state (with _init_nvidia()) or from the control files (with _parse_ctrl_jsons()). After that, the scheduler can begin working through the queued notebooks, controlled by the main() method.\nimport os, subprocess, re, json\n\nclass Scheduler():\n  def __init__(self):\n        self.background_mode = background_mode\n        self.nvidia_base_pids = []\n        self.nvidia_state = {}\n        self._init_nvidia()\n        self.ipynb_names = []\n        self._parse_ctrl_jsons()\n        self.main()\nThe GPU’s initial state needs to be recorded which I’ll do by reading all the processes running on it, and saving these PIDs.\n    def _init_nvidia(self):\n        self._read_nvidia()\n        self.nvidia_base_pids = list(self.nvidia_state.keys())\nFinding these processes takes a little doing. From the command line nvidia-smi produces a nicely formatted text table with this information. I’ve used subprocess to capture this information, and then I’ve parsed the table to get the relevant rows and put the information from each into a dictionary (in the list running_processes). Then each dictionary is saved in the self.nvidia_state under it’s process name.\n    def _read_nvidia(self):    \n        x = subprocess.run(\"nvidia-smi\", shell=True, check=True,  capture_output=True)\n\n        x = str(x).split('\\\\n')\n\n        table_blocks = [i for i in range(len(x)) if re.match('.+===+.+', x[i])]\n        table_breaks = [i for i in range(len(x)) if re.match('.+---+.+', x[i])]\n        process_row  = [i for i in range(len(x)) if re.match('.+ Processes: .+', x[i])]\n        start_line = [i for i in table_blocks if i &gt; process_row[0] ][0]\n        end_line   = [i for i in table_breaks if i &gt; process_row[0] ][0]\n\n        running_processes = [x[i] for i in range(start_line+1, end_line)]\n        running_processes = [dict(zip(\n            ['GPU', 'GI', 'CI', 'PID', 'Type', 'ProcessName', 'GPUMem'],\n            [e for e in line.strip('|').split(' ') if e != ''])) for line in running_processes]\n\n        for e in running_processes:\n            self.nvidia_state[e['PID']] = e\n\n\nReading controls\nNow it needs to read the control files. I’ll identify all the json files in ./SchedulerFiles/ that begin with ‘ctrl’ then run each in turn3. After a file is read the method will check if any of the keys are ‘info’ and return a help message4 if so. Then it will go through each key in order and modify self.nvidia_base_pids or self.ipynb_names accordingly. After a file is processed, it will delete it so that the system doesn’t get trapped in a loop – adding the same notebooks to the queue over and over.\n    def _parse_ctrl_jsons(self):\n        ctrl_files = [e for e in os.listdir('./SchedulerFiles/') if (re.match('ctrl.json', e) or re.match('ctrl\\d+.json', e))]\n        if len(ctrl_files) &gt;= 1:\n            for ctrl_file in ctrl_files:            \n                with open('./SchedulerFiles/'+ctrl_file, 'r') as f:\n                    data = json.load(f)\n\n                keys = tuple(data.keys())\n\n                if 'info' in keys:\n                    print(\"\"\"Text ommited for space\"\"\")\n                for key in keys:\n                    if 'nvidia_base_pids_add' == key:\n                        self.nvidia_base_pids += data[key]\n                    if 'nvidia_base_pids_del' == key:\n                        self.nvidia_base_pids = [e for e in self.nvidia_base_pids if e not in data[key]]\n                    if 'ipynb_names_read' == key:\n                        print(self.ipynb_names)\n                    if 'ipynb_names_add' == key:\n                        self.ipynb_names += data[key]\n                    if 'ipynb_names_next' == key:\n                        # technically this could be used to add files and set them to first\n                        self.ipynb_names = data[key]+[e for e in self.ipynb_names  if e != data[key]]\n                    if 'ipynb_names_del' == key:\n                        self.ipynb_names = [e for e in self.ipynb_names if e != data[key]]\n                \n                # remove the file\n                os.unlink('./SchedulerFiles/'+ctrl_file)            \n\n\nRunning the next job\nNow there’s a way to add jobs to the queue there needs to be a method to run them. This method will check if there’s a file to be run, if it exists, and if so use subprocess to run it in the appropriate conda virtual environment.\n    def _advance_queue(self):\n        if len(self.ipynb_names) == 0:\n            pass\n        else:\n            ipynb_name = self.ipynb_names.pop(0)\n            if os.path.exists(ipynb_name) == False:\n                pass\n            else:\n                process = subprocess.Popen(\n                    f\"conda run -n gpu_env jupyter execute {ipynb_name}\".split(), stdout=subprocess.PIPE\n                    )\n                output, error = process.communicate()\n\n\n\nThe main loop\nNow I can use these methods to process all the items in the queue. As long as there are items to process, it will use _advance_queue() to process the one at the front of the queue. Next it will check if there are any new commands to process. Then it will check if the GPU state matches expectations. If there are any PIDs using the GPU that are not listed in the nvidia_base_pids list these will be stopped. Once the queue is exhausted, the script will stop.\n    def main(self):\n        while len(self.ipynb_names) &gt; 0:\n            print(f'Running {self.ipynb_names[0]}')\n            self._advance_queue()\n            # allow for external controls\n            self._parse_ctrl_jsons()\n            self._read_nvidia()\n            # kill all the processes that were not running at the start. \n            for gpu_pid in [e for e in self.nvidia_state.keys() if e not in self.nvidia_base_pids]:\n                subprocess.run(f'kill -9 {gpu_pid}', shell=True)\n        print('No more queued ipynbs. Exiting.')\n\nAll together (and possible improvements)\nThis system works nicely for how quick it was to write up5. There are plenty of improvements that could be made. Suppose you wanted this to run in the background and idle until you added a new job to the queue. One could imaging changing the main() method to achieve this and extending _parse_ctrl_jsons() to get the system to stop idling and shut down. Or suppose you wanted to queue different file types or run notebooks in different environments – _advance_queue() could be extended to do this. Finally, suppose you don’t want to manually exempt PIDs that aren’t using much of the GPU’s resources. Each PID’s usage is available in the nvidia_state dictionary of dictionaries under GPUMem, so a threshold could be set.\nThese changes and other customization for your use case are left as an exercise for the reader.\nEdit 2023-12-20: I’ve added a background mode and option to begin the main loop directly on initialization.\nimport os, subprocess, re, json,  time\nclass Scheduler():\n    def __init__(self, background_mode = False, run_main = False):\n        self.background_mode = background_mode\n        self.exit = False\n        self.nvidia_base_pids = []\n        self.nvidia_state = {}\n        self._init_nvidia()\n        self.ipynb_names = []\n        if run_main:\n            self.main()\n\n\n    def _init_nvidia(self):\n        self._read_nvidia()\n        self.nvidia_base_pids = list(self.nvidia_state.keys())\n\n    def _read_nvidia(self):    \n        x = subprocess.run(\"nvidia-smi\", shell=True, check=True,  capture_output=True)\n\n        x = str(x).split('\\\\n')\n\n        table_blocks = [i for i in range(len(x)) if re.match('.+===+.+', x[i])]\n        table_breaks = [i for i in range(len(x)) if re.match('.+---+.+', x[i])]\n        process_row  = [i for i in range(len(x)) if re.match('.+ Processes: .+', x[i])]\n        start_line = [i for i in table_blocks if i &gt; process_row[0] ][0]\n        end_line   = [i for i in table_breaks if i &gt; process_row[0] ][0]\n\n        running_processes = [x[i] for i in range(start_line+1, end_line)]\n        running_processes = [dict(zip(\n            ['GPU', 'GI', 'CI', 'PID', 'Type', 'ProcessName', 'GPUMem'],\n            [e for e in line.strip('|').split(' ') if e != ''])) for line in running_processes]\n\n        for e in running_processes:\n            self.nvidia_state[e['PID']] = e\n    \n    def _parse_ctrl_jsons(self):\n        ctrl_files = [e for e in os.listdir('./SchedulerFiles/') if (re.match('ctrl.json', e) or re.match('ctrl\\d+.json', e))]\n        if len(ctrl_files) &gt;= 1:\n            for ctrl_file in ctrl_files:            \n                with open('./SchedulerFiles/'+ctrl_file, 'r') as f:\n                    data = json.load(f)\n\n                keys = tuple(data.keys())\n\n                if 'info' in keys:\n                    print(\"\"\"\nThis scheduling tool uses json files to modify its state while running. \nIt will look for json files beginning with 'ctrl' and containing 0 or more digits in \n./SchedulerFiles/ and then run each. This json should be interpretable as a python dictionary.\nFiles are interpreted in the order of the keys but conflicting orders are not recommended. \nExample file:\n{\n    'info'                :[],                            -&gt; Print this message\n    'nvidia_base_pids_add':['40082'],                     -&gt; Prevent a specific PID from being autoclosed. (e.g. if you're running a gpu session interactively)\n    'nvidia_base_pids_del':['40082'],                     -&gt; Allow a specific PID to be autoclosed.\n    'ipynb_names_read'    :[],                            -&gt; Print currently queued notebooks.\n    'ipynb_names_add'     :['SchedulerTestScript.ipynb'], -&gt; Add a notebook (to the end) of the queue\n    'ipynb_names_next'    :['SchedulerTestScript.ipynb'], -&gt; Add a notebook to the beginning of the queue (does not need to be in the queue)\n    'ipynb_names_del'     :['SchedulerTestScript.ipynb'], -&gt; Remove a notebook from the queue\n    'background_mode'     :['True'],                      -&gt; Set to idle if there are no notebooks in the queue\n    'exit'                :[],                            -&gt; Remove a notebook from the queue\n                          \n}\"\"\")\n                for key in keys:\n                    if 'nvidia_base_pids_add' == key:\n                        self.nvidia_base_pids += data[key]\n                    if 'nvidia_base_pids_del' == key:\n                        self.nvidia_base_pids = [e for e in self.nvidia_base_pids if e not in data[key]]\n                    if 'ipynb_names_read' == key:\n                        print(self.ipynb_names)\n                    if 'ipynb_names_add' == key:\n                        self.ipynb_names += data[key]\n                    if 'ipynb_names_next' == key:\n                        # technically this could be used to add files and set them to first\n                        self.ipynb_names = data[key]+[e for e in self.ipynb_names  if e != data[key]]\n                    if 'ipynb_names_del' == key:\n                        self.ipynb_names = [e for e in self.ipynb_names if e != data[key]]\n                    if 'background_mode' == key:\n                        dat = data[key][0]\n                        if type(dat) == str:\n                            if dat.lower() == 'true':\n                                dat = True\n                            elif dat.lower() == 'false':\n                                dat = False\n                            else:\n                                print(f'{dat} not interpretable as True or False')\n                        if type(dat) == bool:\n                            self.background_mode = dat\n                    if 'exit' == key:\n                        self.exit = True\n\n                # remove the file\n                os.unlink('./SchedulerFiles/'+ctrl_file)\n\n    def _advance_queue(self):\n        if len(self.ipynb_names) == 0:\n            pass\n        else:\n            ipynb_name = self.ipynb_names.pop(0)\n            if os.path.exists(ipynb_name) == False:\n                pass\n            else:\n                process = subprocess.Popen(\n                    f\"conda run -n fastai jupyter execute {ipynb_name}\".split(), stdout=subprocess.PIPE\n                    )\n                output, error = process.communicate()\n\n      def main(self):\n        while ((len(self.ipynb_names) &gt; 0) or (self.background_mode)):\n            if ((len(self.ipynb_names) == 0) and (self.background_mode)):\n                # if idling in background mode wait to check for new commands. \n                time.sleep(10)\n                # While idling any new gpu PIDs should be ignored.\n                self._init_nvidia()\n\n            if self.exit: break        \n            self._parse_ctrl_jsons()\n\n            if self.exit: break\n            if (len(self.ipynb_names) &gt; 0):\n                print(f'Running {self.ipynb_names[0]}')\n                self._advance_queue()\n\n                # allow for external controls\n                self._parse_ctrl_jsons()\n                if self.exit: break        \n\n                self._read_nvidia()\n                # kill all the processes that were not running at the start. \n                for gpu_pid in [e for e in self.nvidia_state.keys() if e not in self.nvidia_base_pids]:\n                    subprocess.run(f'kill -9 {gpu_pid}', shell=True)\n            print(f'Running {time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())}')\n        print(    f'Exiting {time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())}')\n            \n\n# Example usage: Start in background mode. While in background mode new gpu processes shouldn't be killed.\n# shlr = Scheduler(background_mode = True, run_main=True)"
  },
  {
    "objectID": "posts/DanielKick/231229_scheduling_worse_is_better/index.html#footnotes",
    "href": "posts/DanielKick/231229_scheduling_worse_is_better/index.html#footnotes",
    "title": "Worse is better case study 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIndeed, it is named after that slurm.↩︎\nThis is less useful than running jobs as soon as resources are available. However, it does mean that implementing the system is a lot easier. The goal is to get most of the benefit quickly, then return and replace or extend this system once my needs outgrow it.↩︎\nIf there are over 10 control files then they might not run in the order the user expects (e.g. ctrl10 would be run between ctrl1 and ctrl2) but I don’t anticipate issuing that many commands at once.↩︎\nFor formatting this message is omitted here. It is:\nThis scheduling tool uses json files to modify its state while running. \nIt will look for json files beginning with 'ctrl' and containing 0 or more digits in \n./SchedulerFiles/ and then run each. This json should be interpretable as a python dictionary.\nFiles are interpreted in the order of the keys but conflicting orders are not recommended. \nExample file:\n{\n    'info'                :[],                            -&gt; Print this message\n    'nvidia_base_pids_add':['40082'],                     -&gt; Prevent a specific PID from being autoclosed. (e.g. if you're running a gpu session interactively)\n    'nvidia_base_pids_del':['40082'],                     -&gt; Allow a specific PID to be autoclosed.\n    'ipynb_names_read'    :[],                            -&gt; Print currently queued notebooks.\n    'ipynb_names_add'     :['SchedulerTestScript.ipynb'], -&gt; Add a notebook (to the end) of the queue\n    'ipynb_names_next'    :['SchedulerTestScript.ipynb'], -&gt; Add a notebook to the beginning of the queue (does not need to be in the queue)\n    'ipynb_names_del'     :['SchedulerTestScript.ipynb'], -&gt; Remove a notebook from the queue\n}\n↩︎\nWriting this explanation took longer than writing the code.↩︎"
  },
  {
    "objectID": "posts/DanielKick/221027_hpc_access_running_session/index.html",
    "href": "posts/DanielKick/221027_hpc_access_running_session/index.html",
    "title": "Tips: Open a new Interactive Session in Running Session",
    "section": "",
    "text": "A handy trick with batched processes on an HPC is that you can start an interactive session in a running session. Here’s an example where I needed to check if I was nearing the maximum allowed memory:\nHere I list my active jobs to get the jobid, run bash on that node, and list the processes by memory usage.\n[daniel.kick@Atlas-login-1 BLUP_G]$ squeue -u daniel.kick\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           2491772     atlas   BLUP-W daniel.k  R      39:56      1 Atlas-0025\n[daniel.kick@Atlas-login-1 BLUP_G]$ srun --pty --jobid 2491772 bash\n[daniel.kick@Atlas-0025 BLUP_G]$ htop"
  },
  {
    "objectID": "posts/DanielKick/210609_python_jupyter_plugins/index.html",
    "href": "posts/DanielKick/210609_python_jupyter_plugins/index.html",
    "title": "Tips: Jupyter Plugins",
    "section": "",
    "text": "I came across a handy set of tools for jupyter. There are of extensions for the notebooks that give you access to code snips, autocomplete by default, rendering a notebook as a slide show, and other features. To get it installed within an anaconda virtual environment you may only need to install it with this command:\nconda install -c conda-forge jupyter_contrib_nbextensions\nI not all of the extensions were showing up for me until I also ran these two lines, so it may take a bit of fiddling to get it to run.\njupyter contrib nbextension install --user\njupyter nbextension enable codefolding/main\nHere’s a linkto a page that shows some of these extensions in action."
  },
  {
    "objectID": "posts/DanielKick/230913_filtering_to_save_88pr_space/index.html",
    "href": "posts/DanielKick/230913_filtering_to_save_88pr_space/index.html",
    "title": "Save only what you need",
    "section": "",
    "text": "[1] 7424.908\n\n\n[1] 735.05"
  },
  {
    "objectID": "posts/DanielKick/230913_filtering_to_save_88pr_space/index.html#how-we-got-to-this-point",
    "href": "posts/DanielKick/230913_filtering_to_save_88pr_space/index.html#how-we-got-to-this-point",
    "title": "Save only what you need",
    "section": "How we got to this point:",
    "text": "How we got to this point:\nCollecting data from many sites is expensive but using a biophysical model, many sites and years can be ‘harvested’ in minutes. I’m building a dataset with many cultivars planted across the united states. The problem is that I’m being greedy – I want to have the a day by day account of plant’s growth at ~1,300 locations from 1984-2022, varying cultivar, and planting date.\nIn my initial implementation the results from the model are written to a csv for each location…\n\n\nOh no.\n\nThis file has a boat load of data. It’s a table of 25,243,175 rows by 19 columns – 479,620,325 billion cells. By the end of the experiment much of my hard drive will be taken up by these."
  },
  {
    "objectID": "posts/DanielKick/230913_filtering_to_save_88pr_space/index.html#reclaiming-88-storage-space",
    "href": "posts/DanielKick/230913_filtering_to_save_88pr_space/index.html#reclaiming-88-storage-space",
    "title": "Save only what you need",
    "section": "Reclaiming 88% Storage Space",
    "text": "Reclaiming 88% Storage Space\nAn easy place to cut cells is from redundant or unneeded columns. These are produced by the simulation but the way I have the experiment structured, they aren’t needed after it’s done running.\n# YAGNI isn't just for code\ndf &lt;- df[, c(\n      # Indepenent Variables\n      'ith_lon', 'ith_lat', 'soils_i', 'SowDate', 'Genotype', 'Date',\n      \n      # Dependent Variables\n      'Maize.AboveGround.Wt', 'Maize.LAI', 'yield_Kgha'\n\n      # Redundant or not needed\n      #'X', 'ith_year_start', 'ith_year_end', 'factorial_file', 'CheckpointID', \n      #'SimulationID', 'Experiment', 'FolderName', 'Zone', 'Clock.Today'\n      )]\nThis is an easy way to get rid of over half the cells (down to 47.36%) (and really I should not have saved these in the first place) but we can do better still.\nMany of the rows represent times before planting without any data collected. All rows where Maize.AboveGround.Wt, Maize.LAI, and Maize.AboveGround.Wt are 0 can be dropped. Because so much of the year is out of the growing season this is quite helpful and cuts about half of the observations (20.09%).\nSplitting these data into two tables with independent variables or dependent variables (with a key) gets the total down to 10,602 + 53,530,975 = 53,541,577. Still a lot but only 11.16% of the starting size!\n\n\n\n\n\nData\nSize\nPercent Original\n\n\n\nOriginal\n479620325\n100.00\n\n\nSelect Cols.\n227188575\n47.37\n\n\nFilter Rows\n96355755\n20.09\n\n\nSplit Tables\n53541577\n11.16\n\n\n\n\n\n\nI could probably go even further, but now that each experiment takes up only 482 MB instead of 4.64 GB. Furhter optimization can wait for another day.\nWhile storage space is important (at this scale), another factor for the performance (and quality of life) is reading in the data. Using the basic read.csv function it takes 4 minutes 23 seconds to read in. Using the vroom library instead can read in these data in only 4.04 seconds."
  },
  {
    "objectID": "posts/DanielKick/240624_linux_wsl_ln/index.html",
    "href": "posts/DanielKick/240624_linux_wsl_ln/index.html",
    "title": "Tip: Make your life easier with Symbolic Links in WSL",
    "section": "",
    "text": "Modern Windows machines can give you access to Linux command line tools via the Windows Subsystem for Linux. On launch the present working directory is set to the subsystem’s home rather than a location in the main Window’s file structure.\n$ pwd\n/home/&lt;user&gt;\nThe main file structure is in /mnt/ so if you’re for example, trying to pattern match and zip a few files getting there is a bit of a pain. The quick solution is to create symbolic links (shortcuts) between the two.\nThe syntax for symbolic links is ls -s source target.\n$ ln -s /mnt/c/Users/&lt;UserName&gt;/Desktop Desktop\n$ ln -s /mnt/c/Users/&lt;UserName&gt;/Documents Documents\n$ ln -s /mnt/c/Users/&lt;UserName&gt;/Downloads Downloads\n$ ls\nDesktop Documents  Downloads \nAfter creating these links we cd into them and end up in the main system and keep just a little more momentum than we otherwise would"
  },
  {
    "objectID": "posts/manuscript-kick-and-schulz-2018/index.html",
    "href": "posts/manuscript-kick-and-schulz-2018/index.html",
    "title": "Motor Systems: Variability in neural networks",
    "section": "",
    "text": "https://elifesciences.org/articles/34153"
  },
  {
    "objectID": "curriculum-vitae.html",
    "href": "curriculum-vitae.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "If no file appears, please follow this link."
  },
  {
    "objectID": "hype-doc-2023.html",
    "href": "hype-doc-2023.html",
    "title": "Hype-Doc-2023",
    "section": "",
    "text": "Problem: I have the feeling that I’m not growing technically and not sure how to ensure I 1) keep growing in a direction with utility 2) align that growth with publishable projects.\nPlaces for growth:\n\n\nBasic manipulation and understanding of genomes + gene networks\n\nFile manipulation, downsampling, gene identification, linkage disequalibrium\nGene pathways, GO terms\n\n\n\nModeling:\n\n\nGWAS\n\nCan I perform and interpret GWAS?\nCan I explain the basic theory behind it?\n\n\n\nMLMs\n\nHow do they work under the hood? How are random effects encoded? Can I accurately explain the differences between fixed and random effects?\nCan I understand the matrix notation? Could I puzzle out code from an equation if my life depended on it?\nCan I explain correlation structures and how it factors into model fitting?\nCan I extract and interpret breeding values from BLUPs and BLUEs?\nCan I explain how spatial data can be included into a model?\n\n\n\nPhysiological growth models\n\nDo I understand the basic theory?\nDo I know how to get started using them if I needed to?\n\n\n\nNeural nets\n\nCan I satisfactorally explain the basics of a dnn?\nDo I know when to use more complicated architectures/techniques (BNNs, Auto encoders, variational AEs, RNNs, Transformers, etc)\nWould I be able to implement a more complicated model?\n\nHow much inference can I get from the predictions and inputs?\n\nPDPs, LWRP, Salience, SHAP, LIME, etc.\n\n\n\n\n\nCausality\n\nCan I explain (at a high level) differerent ways to approach causality? (DAGs, residual discontinuity, others?)\n\n\n\nBayesian Statistics\n\nCan I explain the advantages/disadvantages of using bayesian methods vs (RE)ML\nCan I implement models (LM, MLM, BayesC) in STAN, PyMC3, or another related program?\n\n\n\n\n\nData extraction\n\nUAV data\nImage analysis\n\n\n\nSelection\n\nCan I explain the breeders equation and common breeding strategies?\nCan I explain where modeling fits in and adds value\n\n\n\nGoals for this year\nReflections\n\nWhat work do I feel most proud of?\nAre there themes in these projects I should be thinking about?\nWhat do I wish I was doing more/less of?\nWhich of my projects had the effect I wanted, and which didn’t\nWhat could have gone better with a project and what might I want to do differently?\nProjects\n(contributions, components, instights) (impact)\nCollaboration & mentorship\nDesign & documentation\nCompany building\n(things that help the company overall, not just your team/project) Recruiting, improving important processes\nWhat you learned\nOutside of work\nBlog postes, open source, talks/ panels"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a PhD Biologist working as a research geneticist. Currently, my research focuses on using deep learning to predict maize yield from genetic, environmental, and management data using deep learning, machine learning, and statistical modeling. Previously, I focused on compensation of neural circuits to aberrant activity."
  }
]