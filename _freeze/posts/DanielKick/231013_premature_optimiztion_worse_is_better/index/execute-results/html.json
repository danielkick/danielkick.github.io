{
  "hash": "62f8a97ec9287478dad60e848c513a84",
  "result": {
    "markdown": "---\ntitle: \"Worse is better and not doing things \\\"right\\\"\"\nsubtitle: \"Avoid premature optimization\"\nauthor: \"Daniel Kick\"\ndate: \"2023-10-13\"\nimage: \"Alwac_III_computer,_1959.jpg\"\ncategories: \n  - tacit knowledge\n  - beginner\n  - intermediate\n  - code\n  - deep learning\nfreeze: true\n---\n\n\n<!-- https://commons.wikimedia.org/wiki/File:Alwac_III_computer,_1959.jpg -->\n\n\"Worse is better\" is an idea I get a lot of mileage out of. Here's the crux of it:\n\n> It refers to the argument that software quality does not necessarily increase with functionality: that there is a point where less functionality (\"worse\") is a preferable option (\"better\") in terms of practicality and usability. [source](https://en.wikipedia.org/wiki/Worse_is_better)\n\nI find this is useful descriptively^[e.g. If scientific manuscripts with embedded code are valuable for reproducibility, why haven't they become the default? There's a lot of energy needed to switch and all of your collaborators already know word. ü§∑üèº‚Äç‚ôÇ ] but also prescriptively as a way to spend less time doing work that doesn't need to be done. \n\nIn brief the idea is that once you have something that works it's often not worth altering it to make it faster, more efficient, or more elegant ... at least initially. Optimization is important ([example](/posts/DanielKick/230913_filtering_to_save_88pr_space/index.html) but what I'm talking about here _premature optimization_. Avoiding the urge to improve things that aren't the priority can be difficult, especially when you conceptually know what you would change. \n\n\n## Simplified Example \n\nHere's an example: I'm building a network that 'compresses' information. The key idea is that there's a function, `f()`, takes in some number of values and outputs _fewer_ values. We can use this function over and over again to compress the values more and more. Once they're 'compressed' we can do the reverse procedure and get more values until we're back at the starting number. \n\nThere's a catch however, and that's that the function can only output an integer number of values even if the number should be a fraction. It's like this division function. If the `numerator` argument is the number of input values and it will return `numerator/3` values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiv_it <- function(numerator, divisor = 3){\n  res = numerator/divisor\n  res = round(res)\n  return(res)\n}\n\ndiv_it(100)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 33\n```\n:::\n:::\n\n\nBecause it can only return whole numbers, we can't reverse this procedure and always get back the same number -- sometimes we have to add or subtract a little bit. \n\n::: {.cell}\n\n```{.r .cell-code}\ninv_div_it <- function(numerator, divisor = 3){\n  return(numerator*divisor)\n}\n\ninv_div_it(33)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 99\n```\n:::\n\n```{.r .cell-code}\ninv_div_it(33)+1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 100\n```\n:::\n:::\n\n\n\nIf we want to really compress the input (`f(X) |> f(X) |> f(X) |> f(X)` or `f(f(f(f(X))))`) then the number of values at each level would be:\n\n::: {.cell}\n\n```{.r .cell-code}\nvals <- c(100)\nfor(i in 1:4){\n  i_val <- vals[length(vals)]\n  vals[length(vals)+1] <- div_it(i_val) \n}\nvals\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 100  33  11   4   1\n```\n:::\n:::\n\n\nIdeally running the inverse procedure multiple times on the last output above (just one value) would output produce:   \n\n::: {.cell}\n\n```{.r .cell-code}\nvals_reverse <- vals[length(vals):1]\nvals_reverse\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]   1   4  11  33 100\n```\n:::\n:::\n\n\nBut using the inverse function defined above (`inv_div_it()`) we get:\n\n::: {.cell}\n\n```{.r .cell-code}\nrecover_vals <- c(1)\nfor(i in 1:4){\n  i_val <- recover_vals[length(recover_vals)]\n  recover_vals[length(recover_vals)+1] <- inv_div_it(i_val) \n}\nrecover_vals\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  1  3  9 27 81\n```\n:::\n:::\n\n\nTo get back to 100 values we need to add a new value (imagine appending a `1` to an array) sometimes, and drop a value others, or make no change to output other times.\n\n::: {.cell}\n\n```{.r .cell-code}\nadd_vals <- c(1, -1, 0, 1)\n\nrecover_vals <- c(1)\nfor(i in 1:4){\n  i_val <- recover_vals[length(recover_vals)]\n  print(add_vals[i])\n  recover_vals[length(recover_vals)+1] <- inv_div_it(i_val) + add_vals[i]\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n[1] -1\n[1] 0\n[1] 1\n```\n:::\n\n```{.r .cell-code}\nrecover_vals\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]   1   4  11  33 100\n```\n:::\n:::\n\n\nWe could keep track of the remainder each time `f()` is called and use that to figure out when to add or subtract 1. That would be the _elegant and efficient_ solution. We know the desired output (100 values) and the number of times `f()` was called (4) so we could also try changing the numbers in `add_vals` until we have four numbers that. This solution would be _inelegant but still effective_. \n\nIf a piece of code only needs to be a few times then the cost of the time you'd spend optimizing it will probably be worth more than than cost of the time the computer spends running it ([see also](https://xkcd.com/1205/)).\n\nIf the sloppy way to express what you want is good enough then don't worry about it. Good enough now is often better than perfect later.   \n\n\n## Example in Context (`python`)\n\nThe motivating problem behind this write up is that 'compressing' weather data (17 measurements for 365 days) into fewer values. I'm using a variation autoencoder with convolution layers which you can imagine as passing a sliding window over a 17x365 grid and summarizing each windowed chunk to get fewer values. \n\nTo check if the compression is effective, we have to compress 17x365 values down to something smaller (e.g. 17x23), and inflate them back to 17x365 so we can compare the input weather to the output weather. If we can get back the same 17x365 values (or something pretty close) then the comprssion is effective.. \n\n\nFrom the input data's length (days) you can calculate what a convolutional layer's output length will be like so:\n\n```python\ndef L_out_conv1d(\n    L_in = 365, \n    kernel_size=3, stride = 2, padding=1, dilation = 1\n): return ((L_in +2*padding-dilation*(kernel_size-1)-1)/stride)+1\n\nL_out_conv1d(L_in = 365) # 183.0\n\n```\nAnd the same for reversing the operation (with a transposed convolution).\n```python\ndef L_out_convT1d(\n    L_in = 183, \n    kernel_size=3, stride = 2, padding=1, output_padding=0, dilation = 1\n): return (L_in - 1)*stride-2*padding+dilation*(kernel_size-1)+output_padding+1\n\nL_out_convT1d(L_in = 183) # 365.0\n\n```\n\nThe trouble is that if I stack convolution layers the output length can become a fraction, which is forced to an integer, and prevents the reverse operation  from producing the right number. When I use 4 layers the length should be `[365, 183.0, 92.0, 46.5, 23.75]` which as integers is `[365, 183, 92, 46, 23]`. Reversing the operation produces `[23, 45, 89, 177, 353]`. \n\nWe can get back to 365 days by increasing the output's length in _some_ of the transposed convolution layers by adding a non-zero `output_padding`. I don't know how many layers will be best, so I can't hard code these values. I could use the functions above to calculate what when the `output_padding` should be 0 and when it shouldn't (the _elegant_ solution), but that's not what I did. \n\n\nInstead I made a simple disposable neural network just to check if I had the `output_padding`s right by tracking the lengths of the tensor after each layer. \n```python\n# input data. One observation, 17 measurements, 365 days of measurements. \n# It's all 0s because all I care about right now is the dimensions of the data.\nxin = torch.zeros((1, 17, 365))\n\n# Proposed output_padding for each layer in the decoder network\nlayer_output_padding = [0, 0, 0, 0, 0, 0, 0, 0]\n\n# encoder network\nne = nn.ModuleList([\n    nn.Sequential(nn.Conv1d(\n    17, out_channels=17, kernel_size= 3, stride= 2, padding  = 1), nn.BatchNorm1d(\n    17), nn.LeakyReLU())\n    for i in range(len(layer_output_padding))\n])\n\n# Decoder network\nnd = nn.ModuleList([\n    nn.Sequential(nn.ConvTranspose1d(\n    17, 17, \n    kernel_size=3, stride = 2, padding=1, output_padding=layer_output_padding[i]), nn.BatchNorm1d(\n    17), nn.LeakyReLU())\n    for i in range(len(layer_output_padding))\n])\n```\n\nThen I can run this network ...\n\n```python\n# list to store lengths\ntensor_Ls = []\n\n# add the input data's length (days)\ntensor_Ls += [list(xin.shape)[-1]] \n\n# encode data\nfor mod in ne:\n    xin = mod(xin)\n    tensor_Ls += [list(xin.shape)[-1]]\n\n# add the encoded data's \ntensor_Ls += [str(tensor_Ls[-1])]\n\n# decode data\nfor mod in nd:\n    xin = mod(xin)\n    tensor_Ls += [list(xin.shape)[-1]]\n```\n... and look at the first and last value of the list of lengths (`tensor_Ls`) to see if the proposed output paddings will work.\n\n```python\ntensor_Ls[0] == tensor_Ls[-1]\n# False\ntensor_Ls\n# [365, 183, 92, 46, 23, 12, 6, 3, 2, '2', 3, 5, 9, 17, 33, 65, 129, 257]\n\n```\n\nNext I need a way to systematically produce different paddings. For a decoder of four layers I would test paddings `[0, 0, 0, 0], [1, 0, 0, 0], ... [1, 1, 1, 1]` stopping at the first list that works. So I'll write a function to increment `[0, 0, 0, 0]` to `[1, 0, 0, 0]`.\n\n```python\ndef increment_list(\n    in_list = [0, 0, 0, 0],\n    min_value = 0,\n    max_value = 1):\n    # Check that all entries are within min/max\n    if False in [True if e <= max_value else False for e in in_list]:\n        print('Value(s) above maximum!')\n    elif False in [True if e >= min_value else False for e in in_list]:\n        print('Value(s) below minimum!')\n    elif [e for e in in_list if e != max_value] == []:\n        print('List at maximum value!')\n    else:    \n        # start cursor at first non-max value\n        for i in range(len(in_list)):\n            if in_list[i] < max_value:\n                in_list[i] += 1\n                break\n            else:\n                in_list[i] = min_value\n    return(in_list)\n\nincrement_list()\n# [1, 0, 0, 0]\n\n```\n\nThen we can loop through possible paddings until we find one that works or have tried all of them.\n\n```python\n# Proposed output_padding for each layer in the decoder network\nlayer_output_padding = [0, 0, 0, 0, 0, 0, 0, 0]\n\nwhile True:\n    # save a backup of the current padding\n    old_layer_output_padding = layer_output_padding.copy()\n    \n    \n    # ... define and run network here ...\n    \n    \n    # If it did work we're done\n    if True == (tensor_Ls[0] == tensor_Ls[-1]):\n        print('done!')\n        \n    # If the padding _didn't_ work change it\n    else:\n        layer_output_padding = increment_list(\n            in_list = layer_output_padding,\n            min_value = 0,\n            max_value = 1)\n    \n    # If the proposed new padding is the same as the backup, then we have tried all the possible paddings and will stop. \n    if layer_output_padding == old_layer_output_padding: \n        break\n```\n\n\n### All together\n\nHere's the full loop and its output:\n\n```python\n\n# Proposed output_padding for each layer in the decoder network\nlayer_output_padding = [0, 0, 0, 0, 0, 0, 0, 0]\n\nwhile True:\n    # save a backup of the current padding\n    old_layer_output_padding = layer_output_padding.copy()\n    \n    # input data. One observation, 17 measurements, 365 days of measurements. \n    # It's all 0s because all I care about right now is the dimensions of the data.\n    xin = torch.zeros((1, 17, 365))\n\n    # encoder network\n    ne = nn.ModuleList([\n        nn.Sequential(nn.Conv1d(\n        17, out_channels=17, kernel_size= 3, stride= 2, padding  = 1), nn.BatchNorm1d(\n        17), nn.LeakyReLU())\n        for i in range(len(layer_output_padding))\n    ])\n\n    # Decoder network\n    nd = nn.ModuleList([\n        nn.Sequential(nn.ConvTranspose1d(\n        17, 17, \n        kernel_size=3, stride = 2, padding=1, output_padding=layer_output_padding[i]), nn.BatchNorm1d(\n        17), nn.LeakyReLU())\n        for i in range(len(layer_output_padding))\n    ])\n    \n    # list to store lengths\n    tensor_Ls = []\n\n    # add the input data's length (days)\n    tensor_Ls += [list(xin.shape)[-1]] \n\n    # encode data\n    for mod in ne:\n        xin = mod(xin)\n        tensor_Ls += [list(xin.shape)[-1]]\n\n    # add the encoded data's \n    tensor_Ls += [str(tensor_Ls[-1])]\n\n    # decode data\n    for mod in nd:\n        xin = mod(xin)\n        tensor_Ls += [list(xin.shape)[-1]]    \n    \n    # If it did work we're done\n    if True == (tensor_Ls[0] == tensor_Ls[-1]):\n        print('done!')\n        \n    # If the padding _didn't_ work change it\n    else:\n        layer_output_padding = increment_list(\n            in_list = layer_output_padding,\n            min_value = 0,\n            max_value = 1)\n    \n    # If the proposed new padding is the same as the backup, then we have tried all the possible paddings and will stop. \n    if layer_output_padding == old_layer_output_padding: \n        break\n\n# done!\n\nlayer_output_padding  \n# [0, 1, 1, 0, 1, 1, 0, 0]\n\ntensor_Ls\n# [365, 183, 92, 46, 23, 12, 6, 3, 2, '2', 3, 6, 12, 23, 46, 92, 183, 365]\n\n```\nThis may not be as not as _elegant_ or as _efficient_ as it could be, but it doesn't matter. It only takes about 200ms so it's not worth improving unless.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}